{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j1pdgkM2PA9a",
   "metadata": {
    "cellView": "form",
    "id": "j1pdgkM2PA9a"
   },
   "outputs": [],
   "source": [
    "#@title ###### Licensed to the Apache Software Foundation (ASF), Version 2.0 (the \"License\")\n",
    "\n",
    "# Licensed to the Apache Software Foundation (ASF) under one\n",
    "# or more contributor license agreements. See the NOTICE file\n",
    "# distributed with this work for additional information\n",
    "# regarding copyright ownership. The ASF licenses this file\n",
    "# to you under the Apache License, Version 2.0 (the\n",
    "# \"License\"); you may not use this file except in compliance\n",
    "# with the License. You may obtain a copy of the License at\n",
    "#\n",
    "#   http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing,\n",
    "# software distributed under the License is distributed on an\n",
    "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
    "# KIND, either express or implied. See the License for the\n",
    "# specific language governing permissions and limitations\n",
    "# under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "orng0uT9-8iT",
   "metadata": {
    "id": "orng0uT9-8iT"
   },
   "source": [
    "# üî• Wildfire spread with Tensorflow and Vertex AI\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/GoogleCloudPlatform/python-docs-samples/blob/main/people-and-planet-ai/land-cover-classification/README.ipynb)\n",
    "\n",
    "In 2021, wildfires destroyed [7 million acres of wildland](https://www.ncei.noaa.gov/access/monitoring/monthly-report/fire/202113)--roughly the same area as the state of Massachusetts. These wildfires destroyed homes, towns, and people's lives.\n",
    "\n",
    "<img src=\"https://media.cnn.com/api/v1/images/stellar/prod/200908110238-07-wildfires-0907-malden-wa.jpg?q=x_17,y_443,h_876,w_1556,c_crop/h_720,w_1280\"/>\n",
    "<caption><i>Figure. The 2020 Babb Road wildfire destroying a home in Malden, WA</i></caption>\n",
    "\n",
    "For a wildfire to catch hold and spread, a set of conditions must exist in an environment. These conditions have been measured and recorded in multiple sources--sources that are available in Earth Engine. Imagine if you could build a ML model that can predict the likelihood and spread of wildfires!\n",
    "\n",
    "This notebook\n",
    "\n",
    "+ ‚è≤Ô∏è Time estimate: TT hours\n",
    "+ üí∞ Cost estimate: Around \\$DD USD (free if you use \\$300 Cloud credits)\n",
    "\n",
    "üíö This is one of many machine learning how-to samples inspired from real climate solutions aired on the People and Planet AI üé• series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "CQzGEx55Nu3P",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CQzGEx55Nu3P",
    "outputId": "9aa174dd-a2ff-40b5-c47c-0865b1634997"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jsWGZW_fUJjN",
   "metadata": {
    "id": "jsWGZW_fUJjN"
   },
   "source": [
    "## üìí Using this interactive notebook\n",
    "\n",
    "Click the **run** icons ‚ñ∂Ô∏è of each section within this notebook.\n",
    "\n",
    "![Run cell](data/images/run-cell.png)\n",
    "\n",
    "> üí° Alternatively, you can run the currently selected cell with `Ctrl + Enter` (or `‚åò + Enter` in a Mac).\n",
    "\n",
    "This **notebook code lets you train and deploy an ML model** from end-to-end. When you run a code cell, the code runs in the notebook's runtime, so you're not making any changes to your personal computer.\n",
    "\n",
    "> ‚ö†Ô∏è **To avoid any errors**, wait for each section to finish in their order before clicking the next ‚Äúrun‚Äù icon.\n",
    "\n",
    "This sample must be connected to a **Google Cloud project**, but nothing else is needed other than your Google Cloud project.\n",
    "\n",
    "You can use an existing project or you can create a new Cloud project [with cloud credits for free.](https://cloud.google.com/free/docs/gcp-free-tier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "G7-JZgREZHQK",
   "metadata": {
    "id": "G7-JZgREZHQK"
   },
   "source": [
    "## üö¥‚Äç‚ôÄÔ∏è Steps summary\n",
    "\n",
    "This notebook is friendly for _beginner_, _intermediate_, and _advanced_ users of geospatial, data analytics and machine learning.\n",
    "**No prior experience is needed** to dive in.\n",
    "\n",
    "Here's a quick summary of what you‚Äôll go through:\n",
    "\n",
    "1. **üìö Understand the data**:\n",
    "  Go through what we want to achieve and explore the data we want to use as _inputs and outputs_ for our model.\n",
    "\n",
    "1. **üóÇ Create the datasets**:\n",
    "  Discover the shape of the datasets and how they will be transformed for use in training.\n",
    "\n",
    "1. **üß† Train the model**:\n",
    "  Use [Keras and Tensorflow](https://keras.io/about/) to train a model on [Vertex AI](https://cloud.google.com/vertex-ai/docs/).\n",
    "\n",
    "1. **üîÆ Get inferences from the model**:\n",
    "  Use your new ML model to predict the spread of wildfires.\n",
    "\n",
    "1. (Optional) **üõ† Delete the project** to avoid ongoing costs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DgtZrFNdP3lv",
   "metadata": {
    "id": "DgtZrFNdP3lv"
   },
   "source": [
    "## üé¨ Before you begin\n",
    "\n",
    "We first need to install all the requirements for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "hArSnUbubJ0W",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hArSnUbubJ0W",
    "outputId": "e8e9dde0-631d-4625-9aaf-418635baabd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "apache-beam[gcp]==2.41.0\n",
    "earthengine-api==0.1.324\n",
    "folium==0.12.1.post1\n",
    "plotly==5.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "WplVt9PfbCWh",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WplVt9PfbCWh",
    "outputId": "6360fb94-e963-4016-9350-373e288811ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing constraints.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile constraints.txt\n",
    "cachetools==4.2.4 # apache-beam requires cachetools<5\n",
    "fastavro==1.5.4\n",
    "fasteners==0.17.3\n",
    "google-api-python-client==1.12.11 # earthengine-api requires google-api-python-client<2\n",
    "google-apitools==0.5.31 # apache-beam requires google-apitools<0.5.32\n",
    "google-auth-httplib2==0.1.0\n",
    "google-auth==1.35.0 # earthengine-api requires google-api-python-client<2\n",
    "google-cloud-bigquery-storage==2.13.2 # apache-beam requires google-cloud-bigquery-storage<2.14\n",
    "google-cloud-bigquery==2.34.4 # apache-beam requires google-cloud-bigquery<3\n",
    "google-cloud-bigtable==1.7.2 # apache-beam requires google-cloud-bigtable<2\n",
    "google-cloud-core==2.3.2\n",
    "google-cloud-datastore==1.15.5 # apache-beam requires google-cloud-datastore<2\n",
    "google-cloud-dlp==3.8.0\n",
    "google-cloud-language==1.3.2 # apache-beam requires google-cloud-language<2\n",
    "google-cloud-pubsub==2.13.5\n",
    "google-cloud-pubsublite==1.4.2\n",
    "google-cloud-storage==2.5.0\n",
    "google-cloud-spanner==1.19.3 # apache-beam requires google-cloud-spanner<2\n",
    "google-cloud-resource-manager==1.6.1\n",
    "google-cloud-recommendations-ai==0.7.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "GHNPnSZybYA5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GHNPnSZybYA5",
    "outputId": "1a2aab74-d55e-434b-d0db-d53a4dda3240"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m93.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m242.1/242.1 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m15.2/15.2 MB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m140.9/140.9 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m152.0/152.0 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m89.7/89.7 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m515.5/515.5 kB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m206.6/206.6 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m83.6/83.6 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m265.8/265.8 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m173.5/173.5 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m180.2/180.2 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m255.6/255.6 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m435.1/435.1 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m267.7/267.7 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m148.2/148.2 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m234.8/234.8 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m152.9/152.9 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m134.2/134.2 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m107.0/107.0 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m81.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m231.1/231.1 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m115.6/115.6 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for earthengine-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for crcmod (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for google-apitools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for httplib2shim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-cloud-firestore 2.7.3 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0, but you have google-api-core 2.10.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip --quiet install --upgrade pip\n",
    "!pip --quiet install -r requirements.txt -c constraints.txt google-cloud-aiplatform\n",
    "\n",
    "# Restart the runtime by ending the runtime's process\n",
    "exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Rq3k4qzz6c8Q",
   "metadata": {
    "id": "Rq3k4qzz6c8Q"
   },
   "source": [
    "## ‚ö†Ô∏è Restart the runtime\n",
    "\n",
    "Colab already comes with many dependencies pre-loaded.\n",
    "In order to ensure everything runs as expected, we have to **restart the runtime**. This allows Colab to load the latest versions of the libraries.\n",
    "\n",
    "![\"Runtime\" > \"Restart runtime\"](data/images/restart-runtime.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RfaQ4Os7QBNn",
   "metadata": {
    "id": "RfaQ4Os7QBNn"
   },
   "source": [
    "# ‚òÅÔ∏è My Google Cloud resources\n",
    "\n",
    "First, choose the Google Cloud _location_ where you want to run this sample.\n",
    "A good place to start is by choosing your [Google Cloud location](https://cloud.google.com/compute/docs/regions-zones).\n",
    "\n",
    "> ‚ö†Ô∏è Make sure you choose a location\n",
    "> available for all products: [Cloud Storage](https://cloud.google.com/storage/docs/locations),\n",
    "> [Vertex AI](https://cloud.google.com/vertex-ai/docs/general/locations),\n",
    "> [Dataflow](https://cloud.google.com/dataflow/docs/resources/locations), and\n",
    "> [Cloud Run](https://cloud.google.com/run/docs/locations).\n",
    "\n",
    "> üí° Prefer locations that are geographically closer to you with\n",
    "> [low carbon emissions](https://cloud.google.com/sustainability/region-carbon), highlighted with the\n",
    "> ![Leaf](https://cloud.google.com/sustainability/region-carbon/gleaf.svg) icon.\n",
    "\n",
    "Make sure you have followed these steps to configure your Google Cloud project:\n",
    "\n",
    "1. Enable the APIs: _Dataflow, Earth Engine, Vertex AI, and Cloud Run_\n",
    "\n",
    "  <button>\n",
    "\n",
    "  [Click here to enable the APIs](https://console.cloud.google.com/flows/enableapi?apiid=dataflow.googleapis.com,earthengine.googleapis.com,aiplatform.googleapis.com,run.googleapis.com)\n",
    "  </button>\n",
    "\n",
    "1. Create a Cloud Storage bucket in your desired _location_.\n",
    "\n",
    "  <button>\n",
    "\n",
    "  [Click here to create a new Cloud Storage bucket](https://console.cloud.google.com/storage/create-bucket)\n",
    "  </button>\n",
    "\n",
    "1. Register your\n",
    "  [Compute Engine default service account](https://console.cloud.google.com/iam-admin/iam)\n",
    "  on Earth Engine.\n",
    "\n",
    "  <button>\n",
    "\n",
    "  [Click here to register your service account on Earth Engine](https://signup.earthengine.google.com/#!/service_accounts)\n",
    "  </button>\n",
    "\n",
    "Once you have everything ready, you can go ahead and fill in your Google Cloud resources in the following code cell.\n",
    "Make sure you run it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fVz5zhvZ1mM3",
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fVz5zhvZ1mM3",
    "outputId": "bb0a58de-ffcf-4290-a945-843203dd977f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google.colab import auth\n",
    "\n",
    "auth.authenticate_user()\n",
    "\n",
    "# Please fill in these values.\n",
    "project = \"video-erschmid\" #@param {type:\"string\"}\n",
    "bucket = \"erschmid-wildfires\" #@param {type:\"string\"}\n",
    "location = \"us-west1\" #@param {type:\"string\"}\n",
    "\n",
    "# Load values from environment variables if available.\n",
    "project = os.environ.get(\"GOOGLE_CLOUD_PROJECT\", project)\n",
    "bucket = os.environ.get(\"CLOUD_STORAGE_BUCKET\", bucket)\n",
    "location = os.environ.get(\"CLOUD_LOCATION\", location)\n",
    "\n",
    "# Quick input validations.\n",
    "assert project, \"‚ö†Ô∏è Please provide a Google Cloud project ID\"\n",
    "assert bucket, \"‚ö†Ô∏è Please provide a Cloud Storage bucket name\"\n",
    "assert not bucket.startswith('gs://'), f\"‚ö†Ô∏è Please remove the gs:// prefix from the bucket name: {bucket}\"\n",
    "assert location, \"‚ö†Ô∏è Please provide a Google Cloud location\"\n",
    "\n",
    "# Configure gcloud.\n",
    "!gcloud config set project {project}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fUX87ic-uNNI",
   "metadata": {
    "id": "fUX87ic-uNNI"
   },
   "source": [
    "Next, we have to authenticate Earth Engine and initialize it.\n",
    "Since we've already authenticated to this [Colab](https://www.youtube.com/watch?v=rNgswRZ2C1Y) and saved our credentials as the [Google default credentials](https://google-auth.readthedocs.io/en/master/reference/google.auth.html#google.auth.default),\n",
    "we can reuse those credentials for Earth Engine.\n",
    "\n",
    "> üí° Since we're making **large amounts of automated requests to Earth Engine**, we want to use the\n",
    "[high-volume endpoint](https://developers.google.com/earth-engine/cloud/highvolume)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3TV2ZvLH17If",
   "metadata": {
    "id": "3TV2ZvLH17If"
   },
   "outputs": [],
   "source": [
    "import ee\n",
    "import google.auth\n",
    "\n",
    "def ee_init() -> None:\n",
    "    \"\"\"Authenticate and initialize Earth Engine with the default credentials.\"\"\"\n",
    "    # Use the Earth Engine High Volume endpoint.\n",
    "    #   https://developers.google.com/earth-engine/cloud/highvolume\n",
    "    credentials, _ = google.auth.default(\n",
    "        scopes=[\n",
    "            \"https://www.googleapis.com/auth/cloud-platform\",\n",
    "            \"https://www.googleapis.com/auth/earthengine\",\n",
    "        ]\n",
    "    )\n",
    "    ee.Initialize(\n",
    "        credentials,\n",
    "        project=project,\n",
    "        opt_url=\"https://earthengine-highvolume.googleapis.com\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "U9JvRARMaq9r",
   "metadata": {
    "id": "U9JvRARMaq9r"
   },
   "outputs": [],
   "source": [
    "ee_init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6V2XkHeNNMb_",
   "metadata": {
    "id": "6V2XkHeNNMb_"
   },
   "source": [
    "# üìö Understand the data\n",
    "\n",
    "Before we begin, let's consider what we want to achieve and the datasets we chose for that purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GkVANXLpZCnd",
   "metadata": {
    "id": "GkVANXLpZCnd"
   },
   "source": [
    "## üéØ **Goal**: Time series forecasting and image segmentation\n",
    "\n",
    "The goal of our model is to use satellite images to analyze the likelihood and potential spread of wildfires for a given geographical region. The output layer will combine a time series forecast (likelihood of fire to spread) and a classification (on fire or not on fire)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aqGsEZBf6ASC",
   "metadata": {
    "id": "aqGsEZBf6ASC"
   },
   "source": [
    "## üõ∞ Inputs: Satellite images\n",
    "\n",
    "To achieve our goal, we must combine multiple geographical datasets into a single dataset (or map in this case). Each input--also known as \"features\" or \"independent variables\"--will be stored as a single band within the resulting map. The following list shows the datasets used for this example:\n",
    "\n",
    "* **USGS/SRTMGL1_003**: NASA SRTM Digital Elevation 30m\n",
    "* **GRIDMET/DROUGHT**: CONUS Drought Indices\n",
    "* **ECMWF/ERA5/DAILY**: Daily Aggregates - Latest Climate Reanalysis Produced by ECMWF / Copernicus Climate Change Service\n",
    "* **IDAHO_EPSCOR/GRIDMET**: University of Idaho Gridded Surface Meteorological Dataset\n",
    "* **CIESIN/GPWv411/GPW_Population_Density**: Population Density (Gridded Population of the World Version 4.11)\n",
    "\n",
    "The following table shows the model input variables, the source dataset, and the symbols used for variable in our model.\n",
    "\n",
    "| Feature | Original Source | Variable name |\n",
    "| --------|:----------------|:--------------|\n",
    "| Elevation | `USGS/SRTMGL1_003` | `elevation` |\n",
    "| Palmer Drought Severity Index | `GRIDMET/DROUGHT` | `psdi` |\n",
    "| Avg air temperature at 2m height | `ECMWF/ERA5/DAILY` | `mean_2m_air_temperature` |\n",
    "| Total precipitation | `ECMWF/ERA5/DAILY` | `total_precipitation` |\n",
    "| 10m u-component of wind (daily avg) | `ECMWF/ERA5/DAILY` | `u_component_of_wind_10m` |\n",
    "| 10m v-component of wind (daily avg) | `ECMWF/ERA5/DAILY` | `v_component_of_wind_10m'` |\n",
    "|\n",
    "| Precipatation amount | `IDAHO_EPSCOR/GRIDMET` | `pr` |\n",
    "| Specific humidity | `IDAHO_EPSCOR/GRIDMET` | `sph` |\n",
    "| Wind direction | `IDAHO_EPSCOR/GRIDMET` | `th` |\n",
    "| Minimum temperature | `IDAHO_EPSCOR/GRIDMET` | `tmmn` |\n",
    "| Maximum temperature | `IDAHO_EPSCOR/GRIDMET` | `tmmx` |\n",
    "| Wind velocity at 10m | `IDAHO_EPSCOR/GRIDMET` | `vs` |\n",
    "| Energy release component | `IDAHO_EPSCOR/GRIDMET` | `erc` |\n",
    "| Population density (per square km) | `CIESIN/GPWv411/GPW_Population_Density` | `population_density` |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "JNpQO_6UQDOt",
   "metadata": {
    "id": "JNpQO_6UQDOt"
   },
   "outputs": [],
   "source": [
    "INPUTS = {\n",
    "    'USGS/SRTMGL1_003': [\"elevation\"],\n",
    "    'GRIDMET/DROUGHT': [\"psdi\"],\n",
    "    'ECMWF/ERA5/DAILY': [\n",
    "         'mean_2m_air_temperature',\n",
    "         'total_precipitation',\n",
    "         'u_component_of_wind_10m',\n",
    "         'v_component_of_wind_10m'],\n",
    "    'IDAHO_EPSCOR/GRIDMET': [\n",
    "         'pr',\n",
    "         'sph',\n",
    "         'th',\n",
    "         'tmmn',\n",
    "         'tmmx',\n",
    "         'vs',\n",
    "         'erc'],\n",
    "    'CIESIN/GPWv411/GPW_Population_Density': ['population_density'],\n",
    "    'MODIS/006/MOD14A1': ['FireMask']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s_j0UGCkZavj",
   "metadata": {
    "id": "s_j0UGCkZavj"
   },
   "source": [
    "## üó∫ **Outputs**: Land cover map\n",
    "\n",
    "Finally, we need to give the model a set of labels to apply to each section of the map. These labels tell the training program (Tensorflow) what we want to infer from the previous data. In other words, this dataset represents the \"dependent variable\" that our model attempts to predict. For our model, we will use the \"Terra Thermal Anomalies & Fire Daily Global 1km (MODIS/006/MOD14A1)\" map from Earth Engine. We'll use the band `FireMask` provided by the map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a_xR5-c0ZhlN",
   "metadata": {
    "id": "a_xR5-c0ZhlN"
   },
   "outputs": [],
   "source": [
    "LABELS = {\n",
    "    'MODIS/006/MOD14A1': ['FireMask'],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bl0hrMIsrSyL",
   "metadata": {
    "id": "bl0hrMIsrSyL"
   },
   "source": [
    "# üóÇ Explore the datasets\n",
    "\n",
    "We need at least two datasets, a _training_ and a _validation_ dataset, to train our model.\n",
    "They both have contain _examples_ of _inputs_ (features) with their respective _outputs_ (labels), but are used for two very different purposes.\n",
    "\n",
    "The _training dataset_ is what the model uses to learn and adjust itself.\n",
    "It goes through this dataset multiple times, similar to a student studying for an exam.\n",
    "\n",
    "The _validation dataset_ is used like an _exam_.\n",
    "It's important that the validation dataset does **not** include examples found in the training dataset, or the validation will be [biased](https://developers.google.com/machine-learning/crash-course/fairness/types-of-bias).\n",
    "After going through the training dataset, the model will test itself against the validation dataset, which should include data it has not seen (learned from) before.\n",
    "\n",
    "For this sample, we'll fetch data from Earth Engine and use that to create our training and validation datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ppiChnJTaC9a",
   "metadata": {
    "id": "ppiChnJTaC9a"
   },
   "source": [
    "---\n",
    "\n",
    "**Note**\n",
    "\n",
    "The magnitude of the data in these datasets is huge: 16 bands of information for a significant sampling of rectangular areas. Because of the size, it is impractical to run an Apache Beam pipeline locally to build the datasets.\n",
    "\n",
    "(Luckily, we can use Cloud Dataflow for this very reason.)\n",
    "\n",
    "In the following cells, we'll examine some of the code used to compose the dataset. However, when it comes to actually building the datasets, we will dispatch our code as a job in Dataflow using a single, discrete Python file named `create_dataset.py`.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8_H_dHx5EMN8",
   "metadata": {
    "id": "8_H_dHx5EMN8"
   },
   "source": [
    "## üìå Sample training points\n",
    "\n",
    "As mentioned previously, the size of the data we'll use is already really large. Inspecting all of the data to validate it could take years!\n",
    "\n",
    "Instead of looking at all of the data, we'll grab just a handful of geographical map points that are meaningful for detecting wildfires. (Don't worry, we've pre-selected some points already.) This process, sometimes called _exploratory data analysis (EDA)_, allows us to understand the shape and contents of our dataset.\n",
    "\n",
    "The points we select for this analysis must satisfy the following criteria:\n",
    "\n",
    "1. The points cannot be in the ocean.\n",
    "1. Some of the points are on fire and the fire spreads.\n",
    "1. Some of the points are on fire and the fire does not spread.\n",
    "1. Some of the points are not on fire.\n",
    "\n",
    "For each point we select, we need to define a rectangular region around the points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hBjbWgx3V1Yg",
   "metadata": {
    "id": "hBjbWgx3V1Yg"
   },
   "source": [
    "### Get the independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bWHxmQhfdR3f",
   "metadata": {
    "id": "bWHxmQhfdR3f"
   },
   "outputs": [],
   "source": [
    "from google.api_core import retry, exceptions\n",
    "from typing import Dict, Iterable, List, Optional, NamedTuple, Tuple\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import ee\n",
    "import io\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "@retry.Retry(deadline=60 * 20)  # seconds\n",
    "def ee_fetch(url: str) -> bytes:\n",
    "    # If we get \"429: Too Many Requests\" errors, it's safe to retry the request.\n",
    "    # The Retry library only works with `google.api_core` exceptions.\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 429:\n",
    "        raise exceptions.TooManyRequests(response.text)\n",
    "\n",
    "    # Still raise any other exceptions to make sure we got valid data.\n",
    "    response.raise_for_status()\n",
    "    return response.content\n",
    "\n",
    "\n",
    "def get_image(\n",
    "    date: datetime, bands_schema: Dict[str, List[str]], window: timedelta\n",
    ") -> ee.Image:\n",
    "    # if elevation dataset is part of bands_schema, deal with it separately\n",
    "    if 'USGS/SRTMGL1_003' in bands_schema:\n",
    "      elevation = ee.Image('USGS/SRTMGL1_003').select(bands_schema['USGS/SRTMGL1_003'])\n",
    "      bands_schema.pop(\"USGS/SRTMGL1_003\")\n",
    "    else:\n",
    "      elevation = None\n",
    "\n",
    "    # if population dataset is part of bands_schema, deal with it separately\n",
    "    if 'CIESIN/GPWv411/GPW_Population_Density' in bands_schema:\n",
    "      population = [\n",
    "          ee.ImageCollection('CIESIN/GPWv411/GPW_Population_Density')\n",
    "        .filterDate(date.isoformat(), (date + window).isoformat())\n",
    "        .select(bands_schema['CIESIN/GPWv411/GPW_Population_Density'])\n",
    "        .median()\n",
    "      ]\n",
    "      bands_schema.pop(\"CIESIN/GPWv411/GPW_Population_Density\")\n",
    "    else:\n",
    "      population = None\n",
    "\n",
    "    images = [\n",
    "        ee.ImageCollection(collection)\n",
    "        .filterDate(date.isoformat(), (date + window).isoformat())\n",
    "        .select(bands)\n",
    "        .mosaic()\n",
    "        for collection, bands in bands_schema.items()\n",
    "    ]\n",
    "    # add elevation to list\n",
    "    if elevation:\n",
    "      images.append(elevation)\n",
    "    # add population to list\n",
    "    if population:\n",
    "      images.append(population)\n",
    "    return ee.Image(images)\n",
    "\n",
    "def get_input_image(date: datetime) -> ee.Image:\n",
    "    return get_image(date, INPUTS, WINDOW)\n",
    "\n",
    "\n",
    "class Bounds(NamedTuple):\n",
    "    west: float\n",
    "    south: float\n",
    "    east: float\n",
    "    north: float\n",
    "\n",
    "def sample_points(\n",
    "    image: ee.Image, num_points: int, bounds: Bounds, scale: int\n",
    ") -> np.ndarray:\n",
    "    def get_coordinates(point: ee.Feature) -> ee.Feature:\n",
    "        coords = point.geometry().coordinates()\n",
    "        return ee.Feature(None, {\"lat\": coords.get(1), \"lon\": coords.get(0)})\n",
    "\n",
    "    points = image.int().stratifiedSample(\n",
    "        num_points,\n",
    "        region=ee.Geometry.Rectangle(bounds),\n",
    "        scale=scale,\n",
    "        geometries=True,\n",
    "    )\n",
    "    url = points.map(get_coordinates).getDownloadURL(\"CSV\", [\"lat\", \"lon\"])\n",
    "    raw_data = ee_fetch(url)\n",
    "    return np.genfromtxt(io.BytesIO(raw_data), delimiter=\",\", skip_header=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "Jzb1PnimGH3h",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jzb1PnimGH3h",
    "outputId": "a3c5d0c6-05a6-4c14-f78c-e4890283f5db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  46.82463029 -119.91005263]\n",
      " [  46.83465548 -119.88450455]\n",
      " [  46.83260733 -119.91522693]\n",
      " [  46.8340087  -119.81023184]]\n"
     ]
    }
   ],
   "source": [
    "SCALE = 5000\n",
    "WINDOW = timedelta(days=1)\n",
    "\n",
    "START_DATE = datetime(2019, 1, 1)\n",
    "END_DATE = datetime(2020, 1, 1)\n",
    "\n",
    "# Find a patch that corresponds to the 243 Command Fire in Washington State, 2019\n",
    "patch_1 = {\n",
    "    \"datetime\": START_DATE,\n",
    "    \"timedelta\": WINDOW,\n",
    "    \"bounds\": Bounds(\n",
    "        west=-119.92358556551419,\n",
    "        east=-119.79964605135403,\n",
    "        north=46.86159776509677,\n",
    "        south=46.81533182118907),\n",
    "    \"on_fire\": True,\n",
    "}\n",
    "\n",
    "img = get_image(patch_1[\"datetime\"], INPUTS, patch_1[\"timedelta\"])\n",
    "samples = sample_points(img, num_points=4, bounds=patch_1[\"bounds\"], scale=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1JysWJJzdL8q",
   "metadata": {
    "id": "1JysWJJzdL8q"
   },
   "source": [
    "### Get the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "TAOLFHNRVRBa",
   "metadata": {
    "id": "TAOLFHNRVRBa"
   },
   "outputs": [],
   "source": [
    "from typing import Tuple, Iterable\n",
    "from datetime import datetime\n",
    "\n",
    "class Point(NamedTuple):\n",
    "    lat: float\n",
    "    lon: float\n",
    "\n",
    "def get_label_image(date: datetime) -> ee.Image:\n",
    "    return get_image(date, LABELS, WINDOW)\n",
    "\n",
    "def sample_labels(\n",
    "    date: datetime, num_points: int, bounds: Bounds\n",
    ") -> Iterable[Tuple[datetime, Point]]:\n",
    "    image = get_label_image(date)\n",
    "    for lat, lon in sample_points(image, num_points, bounds, SCALE):\n",
    "        yield (date, Point(lat, lon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ZEhvTWxEWGzN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZEhvTWxEWGzN",
    "outputId": "c46568bd-4c23-41ed-db84-dca306fe655a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(datetime.datetime(2019, 1, 1, 0, 0), Point(lat=46.82468418473005, lon=-119.90263254785313))\n",
      "(datetime.datetime(2019, 1, 1, 0, 0), Point(lat=46.82468418473005, lon=-119.81280101944118))\n",
      "(datetime.datetime(2019, 1, 1, 0, 0), Point(lat=46.82468418473005, lon=-119.85771678364715))\n"
     ]
    }
   ],
   "source": [
    "label_img = get_label_image(START_DATE)\n",
    "labels = sample_labels(START_DATE, 4, patch_1[\"bounds\"])\n",
    "\n",
    "for label in labels:\n",
    "  print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NyHZTZbYVBsn",
   "metadata": {
    "id": "NyHZTZbYVBsn"
   },
   "source": [
    "##  ‚úçüèΩ Write NumPy files\n",
    "\n",
    "Finally, we need to write the training examples into files. We chose [compressed NumPy files](https://numpy.org/doc/stable/reference/generated/numpy.savez_compressed.html) for simplicity. We used Apache Beam FileSystems to be able to write into any file system that Beam supports, including Cloud Storage.\n",
    "\n",
    "Before writing the examples, we batch them to create files containing multiple examples, rather than a single file per example. This reduces I/O operations when reading the dataset during training.\n",
    "\n",
    "Here, let's create a batch from a single example, but our data creation pipeline will create larger batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "vIkqyNFYVQZF",
   "metadata": {
    "id": "vIkqyNFYVQZF"
   },
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "import numpy as np\n",
    "\n",
    "class Example(NamedTuple):\n",
    "    inputs: np.ndarray\n",
    "    labels: np.ndarray\n",
    "\n",
    "def write_npz_file(example: Example, file_prefix: str) -> str:\n",
    "    from apache_beam.io.filesystems import FileSystems\n",
    "\n",
    "    filename = FileSystems.join(file_prefix, f\"{uuid.uuid4()}.npz\")\n",
    "    with FileSystems.create(filename) as f:\n",
    "        np.savez_compressed(f, inputs=example.inputs, labels=example.labels)\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lYFG_fL3pyms",
   "metadata": {
    "id": "lYFG_fL3pyms"
   },
   "source": [
    "## PIPELINE STACK\n",
    "\n",
    "```\n",
    "random_dates\n",
    "   num_dates = 250\n",
    "\n",
    "sample_labels()\n",
    "   num_points = 100\n",
    "   bounds\n",
    "   get_label_image()\n",
    "   sample_points()\n",
    "\n",
    "try_get_training_example()\n",
    "    patch_size = 64\n",
    "      get_training_example()\n",
    "        ee_init()\n",
    "        get_input_sequence()\n",
    "          get_input_image()\n",
    "          get_patch_sequence()\n",
    "        get_label_sequence()\n",
    "          get_label_image()\n",
    "          get_patch_sequence()\n",
    "        RETURNS Example()\n",
    "\n",
    "\n",
    "write_npz_file()\n",
    "    output_path\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s_vP_MAbwlhh",
   "metadata": {
    "id": "s_vP_MAbwlhh"
   },
   "source": [
    "## üöà Create the datasets in Dataflow\n",
    "\n",
    "We have all the pieces now, so lets put all together into an\n",
    "[Apache Beam](https://beam.apache.org/) pipeline.\n",
    "Apache Beam allows us to create parallel processing pipelines.\n",
    "\n",
    "For this pipeline, we will create a new file, `create_dataset.py`, that will will bundle along into our job to run on Cloud Dataflow.\n",
    "\n",
    "> üí° For more information on how to use Apache Beam, refer to the\n",
    "> [Tour of Beam](https://beam.apache.org/get-started/tour-of-beam/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Z4YA7C-nR9N4",
   "metadata": {
    "id": "Z4YA7C-nR9N4"
   },
   "source": [
    "# TESTING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "hSRdxu60Wz3g",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hSRdxu60Wz3g",
    "outputId": "a687a024-4d74-4ece-da38-174c5fa8b838"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "!gcloud config set project {project}\n",
    "import os\n",
    "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vSMI_351YXrR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vSMI_351YXrR",
    "outputId": "95f54a96-bf6b-411b-9764-43a3024cc78a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:apache_beam.internal.gcp.auth:Setting socket default timeout to 60 seconds.\n",
      "INFO:apache_beam.internal.gcp.auth:socket default timeout is 60.0 seconds.\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/usr/bin/python3', '-m', 'pip', 'download', '--dest', '/tmp/dataflow-requirements-cache', '-r', '/tmp/tmpjupp5lz1/tmp_requirements.txt', '--exists-action', 'i', '--no-deps', '--implementation', 'cp', '--abi', 'cp39', '--platform', 'manylinux2014_x86_64']\n",
      "INFO:apache_beam.runners.portability.stager:Downloading source distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/usr/bin/python3', '-m', 'pip', 'download', '--dest', '/tmp/tmpsk52ziyc', 'apache-beam==2.41.0', '--no-deps', '--no-binary', ':all:']\n",
      "INFO:apache_beam.runners.portability.stager:Staging SDK sources from PyPI: dataflow_python_sdk.tar\n",
      "INFO:apache_beam.runners.portability.stager:Downloading binary distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/usr/bin/python3', '-m', 'pip', 'download', '--dest', '/tmp/tmpsk52ziyc', 'apache-beam==2.41.0', '--no-deps', '--only-binary', ':all:', '--python-version', '39', '--implementation', 'cp', '--abi', 'cp39', '--platform', 'manylinux1_x86_64']\n",
      "INFO:apache_beam.runners.portability.stager:Staging binary distribution of the SDK from PyPI: apache_beam-2.41.0-cp39-cp39-manylinux1_x86_64.whl\n",
      "INFO:apache_beam.runners.portability.sdk_container_builder:Compressed source files for building sdk container at /tmp/tmpwd2che49/source.tgz\n",
      "INFO:apache_beam.runners.portability.sdk_container_builder:Starting GCS upload to gs://erschmid-wildfires/fire/temp/source-9650ac9d-48ca-4bbd-b26a-590b879775fe.tgz...\n",
      "INFO:apache_beam.runners.portability.sdk_container_builder:Completed GCS upload to gs://erschmid-wildfires/fire/temp/source-9650ac9d-48ca-4bbd-b26a-590b879775fe.tgz.\n",
      "INFO:apache_beam.runners.portability.sdk_container_builder:Building sdk container with Google Cloud Build, this may take a few minutes, you may check build log at https://console.cloud.google.com/cloud-build/builds/696900e9-9720-48e9-be1b-210ee66c5072?project=147301782967\n",
      "INFO:apache_beam.runners.portability.sdk_container_builder:Python SDK container pre-build finished in 384.23 seconds\n",
      "INFO:apache_beam.runners.portability.sdk_container_builder:Python SDK container built and pushed as gcr.io/video-erschmid/fire/beam_python_prebuilt_sdk:9650ac9d-48ca-4bbd-b26a-590b879775fe.\n",
      "INFO:root:Default Python SDK image for environment is apache/beam_python3.9_sdk:2.41.0\n",
      "INFO:root:Using provided Python SDK container image: gcr.io/video-erschmid/fire/beam_python_prebuilt_sdk:9650ac9d-48ca-4bbd-b26a-590b879775fe\n",
      "INFO:root:Python SDK container image set to \"gcr.io/video-erschmid/fire/beam_python_prebuilt_sdk:9650ac9d-48ca-4bbd-b26a-590b879775fe\" for Docker environment\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7f7a7b808ca0> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7f7a7b8094c0> ====================\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Defaulting to the temp_location as staging_location: gs://erschmid-wildfires/fire/temp\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://erschmid-wildfires/fire/temp/beamapp-root-0310220904-920808-9enevb0t.1678486144.921055/pickled_main_session...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://erschmid-wildfires/fire/temp/beamapp-root-0310220904-920808-9enevb0t.1678486144.921055/pickled_main_session in 0 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://erschmid-wildfires/fire/temp/beamapp-root-0310220904-920808-9enevb0t.1678486144.921055/pipeline.pb...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://erschmid-wildfires/fire/temp/beamapp-root-0310220904-920808-9enevb0t.1678486144.921055/pipeline.pb in 0 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Create job: <Job\n",
      " clientRequestId: '20230310220904921756-7645'\n",
      " createTime: '2023-03-10T22:09:06.938222Z'\n",
      " currentStateTime: '1970-01-01T00:00:00Z'\n",
      " id: '2023-03-10_14_09_06-9967128250707529020'\n",
      " location: 'us-west1'\n",
      " name: 'beamapp-root-0310220904-920808-9enevb0t'\n",
      " projectId: 'video-erschmid'\n",
      " stageStates: []\n",
      " startTime: '2023-03-10T22:09:06.938222Z'\n",
      " steps: []\n",
      " tempFiles: []\n",
      " type: TypeValueValuesEnum(JOB_TYPE_BATCH, 1)>\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Created job with id: [2023-03-10_14_09_06-9967128250707529020]\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Submitted job: 2023-03-10_14_09_06-9967128250707529020\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:To access the Dataflow monitoring console, please navigate to https://console.cloud.google.com/dataflow/jobs/us-west1/2023-03-10_14_09_06-9967128250707529020?project=video-erschmid\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2023-03-10_14_09_06-9967128250707529020 is in state JOB_STATE_PENDING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-03-10T22:09:06.375Z: JOB_MESSAGE_BASIC: Dataflow Runner V2 auto-enabled. Use --experiments=disable_runner_v2 to opt out.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-03-10T22:09:08.280Z: JOB_MESSAGE_DETAILED: Autoscaling is enabled for job 2023-03-10_14_09_06-9967128250707529020. The number of workers will be between 1 and 20.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-03-10T22:09:08.343Z: JOB_MESSAGE_DETAILED: Autoscaling was automatically enabled for job 2023-03-10_14_09_06-9967128250707529020.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-03-10T22:09:10.045Z: JOB_MESSAGE_BASIC: Worker configuration: n1-standard-1 in us-west1-a.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-03-10T22:09:10.732Z: JOB_MESSAGE_DETAILED: Expanding SplittableParDo operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-03-10T22:09:10.748Z: JOB_MESSAGE_DETAILED: Expanding CollectionToSingleton operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-03-10T22:09:10.798Z: JOB_MESSAGE_DETAILED: Expanding CoGroupByKey operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-03-10T22:09:10.822Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step Reshuffle/ReshufflePerKey/GroupByKey: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-03-10T22:09:10.844Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/GroupByKey: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-03-10T22:09:10.872Z: JOB_MESSAGE_DETAILED: Expanding GroupByKey operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-03-10T22:09:10.890Z: JOB_MESSAGE_DEBUG: Annotating graph with Autotuner information.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-03-10T22:09:10.909Z: JOB_MESSAGE_DETAILED: Fusing adjacent ParDo, Read, Write, and Flatten operations\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-03-10T22:09:10.931Z: JOB_MESSAGE_DETAILED: Fusing consumer Random dates/FlatMap(<lambda at core.py:3481>) into Random dates/Impulse\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-03-10T22:09:10.955Z: JOB_MESSAGE_DETAILED: Fusing consumer Random dates/MaybeReshuffle/Reshuffle/AddRandomKeys into Random dates/FlatMap(<lambda at core.py:3481>)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-03-10T22:09:10.976Z: JOB_MESSAGE_DETAILED: Fusing consumer Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/Map(reify_timestamps) into Random dates/MaybeReshuffle/Reshuffle/AddRandomKeys\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-03-10T22:09:10.990Z: JOB_MESSAGE_DETAILED: Fusing consumer Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/GroupByKey/Reify into Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/Map(reify_timestamps)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-03-10T22:09:11.010Z: JOB_MESSAGE_DETAILED: Fusing consumer Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/GroupByKey/Write into Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-03-10T22:09:11.023Z: JOB_MESSAGE_DETAILED: Fusing consumer Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/GroupByKey/GroupByWindow into Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-03-10T22:09:11.047Z: JOB_MESSAGE_DETAILED: Fusing consumer Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/FlatMap(restore_timestamps) into Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/GroupByKey/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-03-10T22:09:11.068Z: JOB_MESSAGE_DETAILED: Fusing consumer Random dates/MaybeReshuffle/Reshuffle/RemoveRandomKeys into Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/FlatMap(restore_timestamps)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-03-10T22:09:11.085Z: JOB_MESSAGE_DETAILED: Fusing consumer Random dates/Map(decode) into Random dates/MaybeReshuffle/Reshuffle/RemoveRandomKeys\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-03-10T22:09:11.109Z: JOB_MESSAGE_DETAILED: Fusing consumer Sample labels into Random dates/Map(decode)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-03-10T22:09:11.124Z: JOB_MESSAGE_DETAILED: Fusing consumer Reshuffle/AddRandomKeys into Sample labels\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-03-10T22:09:11.148Z: JOB_MESSAGE_DETAILED: Fusing consumer Reshuffle/ReshufflePerKey/Map(reify_timestamps) into Reshuffle/AddRandomKeys\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-03-10T22:09:11.171Z: JOB_MESSAGE_DETAILED: Fusing consumer Reshuffle/ReshufflePerKey/GroupByKey/Reify into Reshuffle/ReshufflePerKey/Map(reify_timestamps)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-03-10T22:09:11.195Z: JOB_MESSAGE_DETAILED: Fusing consumer Reshuffle/ReshufflePerKey/GroupByKey/Write into Reshuffle/ReshufflePerKey/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-03-10T22:09:11.218Z: JOB_MESSAGE_DETAILED: Fusing consumer Reshuffle/ReshufflePerKey/GroupByKey/GroupByWindow into Reshuffle/ReshufflePerKey/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-03-10T22:09:11.243Z: JOB_MESSAGE_DETAILED: Fusing consumer Reshuffle/ReshufflePerKey/FlatMap(restore_timestamps) into Reshuffle/ReshufflePerKey/GroupByKey/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-03-10T22:09:11.266Z: JOB_MESSAGE_DETAILED: Fusing consumer Reshuffle/RemoveRandomKeys into Reshuffle/ReshufflePerKey/FlatMap(restore_timestamps)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-03-10T22:09:11.289Z: JOB_MESSAGE_DETAILED: Fusing consumer Get example into Reshuffle/RemoveRandomKeys\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-03-10T22:09:11.312Z: JOB_MESSAGE_DETAILED: Fusing consumer Write NPZ files into Get example\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-03-10T22:09:11.335Z: JOB_MESSAGE_DETAILED: Fusing consumer Log files into Write NPZ files\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-03-10T22:09:11.366Z: JOB_MESSAGE_DEBUG: Workflow config is missing a default resource spec.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-03-10T22:09:11.385Z: JOB_MESSAGE_DEBUG: Adding StepResource setup and teardown to workflow graph.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-03-10T22:09:11.409Z: JOB_MESSAGE_DEBUG: Adding workflow start and stop steps.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-03-10T22:09:11.440Z: JOB_MESSAGE_DEBUG: Assigning stage ids.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-03-10T22:09:11.528Z: JOB_MESSAGE_DEBUG: Executing wait step start23\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-03-10T22:09:11.585Z: JOB_MESSAGE_BASIC: Executing operation Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-03-10T22:09:11.636Z: JOB_MESSAGE_DEBUG: Starting worker pool setup.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-03-10T22:09:11.654Z: JOB_MESSAGE_BASIC: Starting 1 workers in us-west1-a...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-03-10T22:09:11.927Z: JOB_MESSAGE_BASIC: Finished operation Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-03-10T22:09:11.975Z: JOB_MESSAGE_DEBUG: Value \"Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-03-10T22:09:12.033Z: JOB_MESSAGE_BASIC: Executing operation Random dates/Impulse+Random dates/FlatMap(<lambda at core.py:3481>)+Random dates/MaybeReshuffle/Reshuffle/AddRandomKeys+Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/Map(reify_timestamps)+Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/GroupByKey/Reify+Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2023-03-10_14_09_06-9967128250707529020 is in state JOB_STATE_RUNNING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-03-10T22:09:54.887Z: JOB_MESSAGE_DETAILED: Autoscaling: Raised the number of workers to 1 based on the rate of progress in the currently running stage(s).\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-03-10T22:12:40.294Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-03-10T22:12:59.435Z: JOB_MESSAGE_DETAILED: All workers have finished the startup processes and began to receive work requests.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-03-10T22:13:01.934Z: JOB_MESSAGE_BASIC: Finished operation Random dates/Impulse+Random dates/FlatMap(<lambda at core.py:3481>)+Random dates/MaybeReshuffle/Reshuffle/AddRandomKeys+Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/Map(reify_timestamps)+Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/GroupByKey/Reify+Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-03-10T22:13:01.978Z: JOB_MESSAGE_BASIC: Executing operation Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-03-10T22:13:02.077Z: JOB_MESSAGE_BASIC: Finished operation Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-03-10T22:13:02.125Z: JOB_MESSAGE_BASIC: Executing operation Reshuffle/ReshufflePerKey/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-03-10T22:13:02.263Z: JOB_MESSAGE_BASIC: Finished operation Reshuffle/ReshufflePerKey/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-03-10T22:13:02.310Z: JOB_MESSAGE_DEBUG: Value \"Reshuffle/ReshufflePerKey/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-03-10T22:13:02.354Z: JOB_MESSAGE_BASIC: Executing operation Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/GroupByKey/Read+Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/GroupByKey/GroupByWindow+Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/FlatMap(restore_timestamps)+Random dates/MaybeReshuffle/Reshuffle/RemoveRandomKeys+Random dates/Map(decode)+Sample labels+Reshuffle/AddRandomKeys+Reshuffle/ReshufflePerKey/Map(reify_timestamps)+Reshuffle/ReshufflePerKey/GroupByKey/Reify+Reshuffle/ReshufflePerKey/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-03-10T22:14:25.001Z: JOB_MESSAGE_DETAILED: Autoscaling: Resizing worker pool from 1 to 3.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-03-10T22:14:30.203Z: JOB_MESSAGE_DETAILED: Autoscaling: Raised the number of workers to 3 based on the rate of progress in the currently running stage(s).\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-03-10T22:18:57.684Z: JOB_MESSAGE_DETAILED: Autoscaling: Reduced the number of workers to 2 based on the rate of progress in the currently running stage(s).\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-03-10T22:19:02.702Z: JOB_MESSAGE_DETAILED: Autoscaling: Resizing worker pool from 3 to 2.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-03-10T22:20:08.597Z: JOB_MESSAGE_BASIC: Finished operation Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/GroupByKey/Read+Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/GroupByKey/GroupByWindow+Random dates/MaybeReshuffle/Reshuffle/ReshufflePerKey/FlatMap(restore_timestamps)+Random dates/MaybeReshuffle/Reshuffle/RemoveRandomKeys+Random dates/Map(decode)+Sample labels+Reshuffle/AddRandomKeys+Reshuffle/ReshufflePerKey/Map(reify_timestamps)+Reshuffle/ReshufflePerKey/GroupByKey/Reify+Reshuffle/ReshufflePerKey/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-03-10T22:20:08.639Z: JOB_MESSAGE_BASIC: Executing operation Reshuffle/ReshufflePerKey/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-03-10T22:20:08.679Z: JOB_MESSAGE_BASIC: Finished operation Reshuffle/ReshufflePerKey/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-03-10T22:20:08.717Z: JOB_MESSAGE_BASIC: Executing operation Reshuffle/ReshufflePerKey/GroupByKey/Read+Reshuffle/ReshufflePerKey/GroupByKey/GroupByWindow+Reshuffle/ReshufflePerKey/FlatMap(restore_timestamps)+Reshuffle/RemoveRandomKeys+Get example+Write NPZ files+Log files\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-03-10T22:21:28.886Z: JOB_MESSAGE_DETAILED: Autoscaling: Resizing worker pool from 2 to 20.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2023-03-10T22:21:34.058Z: JOB_MESSAGE_DETAILED: Autoscaling: Raised the number of workers to 20 based on the rate of progress in the currently running stage(s).\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "from datetime import datetime\n",
    "bounds = [-124, 24, -73, 49]\n",
    "date = datetime(2020, 1, 1)\n",
    "output_path = \"gs://{bucket}/fire/large_data\"\n",
    "num_dates = \"250\"\n",
    "num_points = \"100\"\n",
    "runner = \"DataflowRunner\"\n",
    "region = \"{location}\"\n",
    "temp_location = \"gs://{bucket}/fire/temp\"\n",
    "prebuild_sdk_container_engine = \"cloud_build\"\n",
    "docker_registry_push_url = \"gcr.io/{project}/fire\"\n",
    "\n",
    "!python create_dataset.py \\\n",
    "  --output-path \"gs://{bucket}/fire/large_data\" \\\n",
    "  --num-dates \"250\" \\\n",
    "  --num-points \"100\" \\\n",
    "  --runner \"DataflowRunner\" \\\n",
    "  --project \"{project}\" \\\n",
    "  --region \"{location}\" \\\n",
    "  --temp_location \"gs://{bucket}/fire/temp\" \\\n",
    "  --prebuild_sdk_container_engine \"cloud_build\" \\\n",
    "  --docker_registry_push_url \"gcr.io/{project}/fire\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ejZG4kzpRr2K",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ejZG4kzpRr2K",
    "outputId": "9569ec9a-0363-4d9c-ac1e-0e9b14e3daef"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:File `'(output_path=output_path,.py'` not found.\n"
     ]
    }
   ],
   "source": [
    "run(output_path=output_path, num_dates=num_date, num_points=num_points, bounds=Bounds(-124, 24, -73, 49), patch_size=64, max_request=20, beam_args=beam_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SwC3a7nVVauo",
   "metadata": {
    "id": "SwC3a7nVVauo"
   },
   "source": [
    "# üß† Train the model\n",
    "\n",
    "We're now entering into the realms of PyTorch and linear algebra.\n",
    "**Do not worry, we'll try to keep it simple**.\n",
    "\n",
    "The overall process of training the model is _pretty straightforward_, but there's a lot of things to know on how to do things.\n",
    "\n",
    "First, some basic definitions:\n",
    "\n",
    "- üîÆ **Model**: You can think of the model as a function. You give it some inputs and it returns you some outputs. But rather than writing the function yourself, it learns from examples.\n",
    "- üßÆ **Tensor**: Numbers without dimensions are _scalars_, 1-dimensional numbers are _vectors_, 2-dimensional numbers are _matrices_, multi-dimensional numbers are _tensors_. Machine learning models take tensors as inputs and return you tensors.\n",
    "- üèóÔ∏è **Shape**: This says how many dimensions and how they look. For example, a 3x4 matrix has shape `(3, 4)`, and an 800x600 RGB image has a shape `(800, 600, 3)` since we have three numbers (red, green, blue) for every pixel."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cu110.m103",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m103"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
