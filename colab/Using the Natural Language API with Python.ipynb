{"cells":[{"cell_type":"markdown","metadata":{"id":"yxKFLozC5tfO"},"source":["```text\n","SPDX-FileCopyrightText: 2023 Google LLC\n","SPDX-License-Identifier: Apache-2.0\n","```\n","\n","# ü§Ø Using the Natural Language API with Python\n","\n","<center>\n","<table><tr><td>\n","<img src=\"pics/natural_language_api.png\" style=\"height:200px\" height=\"200\" />\n","</td></tr></table>\n","<table><tr>\n","<td><a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/python-docs-samples/blob/main/colab/Using%20the%20Natural%20Language%20API%20with%20Python.ipynb\">\n","<img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\" align=\"center\"> Run in Colab\n","</a></td>\n","<td><a href=\"https://github.com/GoogleCloudPlatform/python-docs-samples/blob/main/colab/Using%20the%20Natural%20Language%20API%20with%20Python.ipynb\">\n","<img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\" align=\"center\"> View on GitHub\n","</a></td>\n","</tr></table>\n","</center>\n","\n","The [Natural Language API](https://cloud.google.com/natural-language/docs/) lets you extract information from unstructured text using Google machine learning. In this tutorial, you'll focus on using its Python client library to perform the following:\n","\n","- Sentiment analysis\n","- Entity analysis\n","- Syntax analysis\n","- Content classification\n","\n","This notebook requires a Google Cloud project:\n","\n","- If needed, [create a new Google Cloud project](https://console.cloud.google.com/cloud-resource-manager).\n","- Make sure that billing is enabled for your project.\n","- It uses billable services but not should generate any cost (see the Natural Language API [free monthly thresholds](https://cloud.google.com/natural-language/pricing)).\n","\n","> This port to Colab was originally published on [Google Developers Codelabs](https://codelabs.developers.google.com/codelabs/cloud-natural-language-python3).\n","\n","---\n","\n","## ‚öôÔ∏è Project ID\n","\n","- Define your project ID.\n","- Launch _Runtime_ > _Run all_.\n","- The first time, you'll need to allow access to your Google credentials. Select your Google Cloud account. It should have rights to this project.\n","- If the setup step installs packages and restarts, launch \"Run all\" again.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2IISlEphcCGt"},"outputs":[],"source":["PROJECT_ID = \"\"  # @param {type:\"string\"}\n","\n","assert PROJECT_ID, \"‚ùå Please enter your project ID\"\n","\n","print(f'‚úîÔ∏è PROJECT_ID: \"{PROJECT_ID}\"')"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"JAUW8f9oi8mi"},"source":["---\n","\n","## ‚úîÔ∏è Setup\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mn2UJ8_VUtFa"},"outputs":[],"source":["# @title a. Check packages (may restart) {display-mode: \"form\"}\n","import sys\n","from importlib.metadata import PackageNotFoundError, version\n","\n","from IPython.core.getipython import get_ipython\n","\n","# Services needed for this lab (with minimum major version)\n","GOOGLE_CLOUD_SERVICES = [\n","    (\"language\", 2),\n","]\n","APIS = [f\"{service}.googleapis.com\" for service, _ in GOOGLE_CLOUD_SERVICES]\n","\n","# Check runtime\n","running_in_colab = \"google.colab\" in sys.modules\n","assert running_in_colab, \"‚ùå The notebook was not tested outside of Colab\"\n","print(\"‚úîÔ∏è Running in Colab\")\n","\n","# Check packages\n","packages = []\n","for service, min_major in GOOGLE_CLOUD_SERVICES:\n","    package = f\"google-cloud-{service}\"\n","    lib = f\"google.cloud.{service}\"\n","    try:\n","        lib_version = version(lib)\n","        lib_major = int(lib_version.split(\".\")[0])\n","        if min_major <= lib_major:\n","            print(f\"‚úîÔ∏è {lib}=={lib_version}\")\n","            # Note: Assumes (min_major < lib_major) versions are non-breaking\n","            continue\n","        packages.append(package)\n","        print(f\"üì¶Ô∏è {package} to be updated‚Ä¶\")\n","    except PackageNotFoundError:\n","        packages.append(package)\n","        print(f\"üì¶Ô∏è {package} to be installed‚Ä¶\")\n","\n","if packages:\n","    # Install and restart\n","    requirements = \" \".join(packages)\n","    %pip install --upgrade $requirements --quiet\n","    if instance := get_ipython():\n","        instance.kernel.do_shutdown(True)\n","    raise RuntimeWarning(\"üîÑ Restarting‚Ä¶ (run the cell again)\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5Xt81s9pOlfI"},"outputs":[],"source":["# @title b. Check authentication {display-mode: \"form\"}\n","from google.colab import auth as google_auth\n","\n","google_auth.authenticate_user(project_id=PROJECT_ID)\n","print(f\"‚úîÔ∏è Authenticated\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w-QXa96aXzgw"},"outputs":[],"source":["# @title c. Check project APIs {display-mode: \"form\"}\n","res = !gcloud services list --enabled --format \"value(config.name)\"\n","\n","apis_to_enable = \"\"\n","for api in APIS:\n","    if api in res:\n","        print(f'‚úîÔ∏è API \"{api}\" is enabled')\n","    else:\n","        apis_to_enable += f\"{api} \"\n","\n","if apis_to_enable:\n","    print(f'üîì Enabling API \"{apis_to_enable}\"‚Ä¶')\n","    !gcloud services enable $api\n","elif not APIS:\n","    print(f\"‚úîÔ∏è No specific API needed\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8zA4DxGDagbv"},"outputs":[],"source":["# @title d. Define helpers {display-mode: \"form\"}\n","import pandas as pd\n","from IPython.display import display\n","\n","\n","def show_table(columns, data, formats=None, remove_empty_columns=False):\n","    df = pd.DataFrame(columns=columns, data=data)\n","    if remove_empty_columns:\n","        empty_cols = [col for col in df if df[col].eq(\"\").all()]\n","        df.drop(empty_cols, axis=1, inplace=True)\n","    # Customize formatting\n","    styler = df.style\n","    if formats:\n","        styler.format(formats)\n","    # Left-align string columns\n","    df = df.convert_dtypes()\n","    str_cols = list(df.select_dtypes(\"string\").keys())\n","    styler = styler.set_properties(subset=str_cols, **{\"text-align\": \"left\"})\n","    # Center headers\n","    styler.set_table_styles([{\"selector\": \"th\", \"props\": [(\"text-align\", \"center\")]}])\n","    styler.hide()\n","    display(styler)\n","\n","\n","print(f\"‚úîÔ∏è Helpers defined\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"YuhPWlD5M1Kp"},"source":["---\n","\n","## üêç Using the Python client library\n","\n","You can use the Natural Language API in Python with the client library `google-cloud-language`. The previous step already checked its installation. Import the client library:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vKysAffRWGnv"},"outputs":[],"source":["from google.cloud import language"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"o83udK7BZOBC"},"source":["---\n","\n","## 1Ô∏è‚É£ Sentiment analysis\n","\n","Sentiment analysis inspects the given text and identifies the prevailing emotional opinions within the text, especially to determine expressed sentiments as positive, negative, or neutral, both at the sentence and the document levels. It is performed with the `analyze_sentiment` method which returns an `AnalyzeSentimentResponse`.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ELEmIsYJZQ-i"},"outputs":[],"source":["def analyze_text_sentiment(text: str) -> language.AnalyzeSentimentResponse:\n","    client = language.LanguageServiceClient()\n","    document = language.Document(\n","        content=text,\n","        type_=language.Document.Type.PLAIN_TEXT,\n","    )\n","    return client.analyze_sentiment(document=document)\n","\n","\n","def show_text_sentiment(response: language.AnalyzeSentimentResponse):\n","    columns = [\"score\", \"sentence\"]\n","    data = [(s.sentiment.score, s.text.content) for s in response.sentences]\n","    formats = {\"score\": \"{:+.1f}\"}\n","    print(\"At sentence level:\")\n","    show_table(columns, data, formats)\n","\n","    sentiment = response.document_sentiment\n","    columns = [\"score\", \"magnitude\", \"language\"]\n","    data = [(sentiment.score, sentiment.magnitude, response.language)]\n","    formats = {\"score\": \"{:+.1f}\", \"magnitude\": \"{:.1f}\"}\n","    print(\"\")\n","    print(\"At document level:\")\n","    show_table(columns, data, formats)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"AB2X-SKMwjLU"},"source":["Now, let's perform an analysis:\n"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":259},"executionInfo":{"elapsed":576,"status":"ok","timestamp":1686139226818,"user":{"displayName":"Laurent Picard (GCP Demos)","userId":"15587849602996756367"},"user_tz":-120},"id":"M9mzah76Y0gl","outputId":"9ecb2916-02d5-431e-bb0d-246b4b4d1543"},"outputs":[{"name":"stdout","output_type":"stream","text":["At sentence level:\n"]},{"data":{"text/html":["<style type=\"text/css\">\n","#T_b3095 th {\n","  text-align: center;\n","}\n","#T_b3095_row0_col1, #T_b3095_row1_col1, #T_b3095_row2_col1 {\n","  text-align: left;\n","}\n","</style>\n","<table id=\"T_b3095\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th id=\"T_b3095_level0_col0\" class=\"col_heading level0 col0\" >score</th>\n","      <th id=\"T_b3095_level0_col1\" class=\"col_heading level0 col1\" >sentence</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td id=\"T_b3095_row0_col0\" class=\"data row0 col0\" >+0.8</td>\n","      <td id=\"T_b3095_row0_col1\" class=\"data row0 col1\" >Python is a very readable language, which makes it easy to understand and maintain code.</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_b3095_row1_col0\" class=\"data row1 col0\" >+0.9</td>\n","      <td id=\"T_b3095_row1_col1\" class=\"data row1 col1\" >It's simple, very flexible, easy to learn, and suitable for a wide variety of tasks.</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_b3095_row2_col0\" class=\"data row2 col0\" >-0.4</td>\n","      <td id=\"T_b3095_row2_col1\" class=\"data row2 col1\" >One disadvantage is its speed: it's not as fast as some other programming languages.</td>\n","    </tr>\n","  </tbody>\n","</table>\n"],"text/plain":["<pandas.io.formats.style.Styler at 0x7f25d1eebcd0>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","At document level:\n"]},{"data":{"text/html":["<style type=\"text/css\">\n","#T_2312b th {\n","  text-align: center;\n","}\n","#T_2312b_row0_col2 {\n","  text-align: left;\n","}\n","</style>\n","<table id=\"T_2312b\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th id=\"T_2312b_level0_col0\" class=\"col_heading level0 col0\" >score</th>\n","      <th id=\"T_2312b_level0_col1\" class=\"col_heading level0 col1\" >magnitude</th>\n","      <th id=\"T_2312b_level0_col2\" class=\"col_heading level0 col2\" >language</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td id=\"T_2312b_row0_col0\" class=\"data row0 col0\" >+0.4</td>\n","      <td id=\"T_2312b_row0_col1\" class=\"data row0 col1\" >2.2</td>\n","      <td id=\"T_2312b_row0_col2\" class=\"data row0 col2\" >en</td>\n","    </tr>\n","  </tbody>\n","</table>\n"],"text/plain":["<pandas.io.formats.style.Styler at 0x7f25ee60a4a0>"]},"metadata":{},"output_type":"display_data"}],"source":["# Input\n","text = \"Python is a very readable language, which makes it easy to understand and maintain code. It's simple, very flexible, easy to learn, and suitable for a wide variety of tasks. One disadvantage is its speed: it's not as fast as some other programming languages.\"  # @param {type:\"string\"}\n","\n","# Send a request to the API\n","analyze_sentiment_response = analyze_text_sentiment(text)\n","\n","# Show the results\n","show_text_sentiment(analyze_sentiment_response)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"7hU8WO15c0jn"},"source":["Notes:\n","\n","- For information on which languages are supported by the Natural Language API, see [Language Support](https://cloud.google.com/natural-language/docs/languages#sentiment_analysis).\n","- The `score` of the sentiment ranges between -1.0 (negative) and 1.0 (positive) and corresponds to the overall sentiment from the given information.\n","- The `magnitude` of the sentiment ranges from 0.0 to +infinity and indicates the overall strength of sentiment from the given information. The more information provided, the higher the magnitude.\n","- For more information on how to interpret the `score` and `magnitude` sentiment values included in the analysis, see [Interpreting sentiment analysis values](https://cloud.google.com/natural-language/docs/basics#interpreting_sentiment_analysis_values).\n","- Each API response returns the document automatically-detected language (in ISO-639-1). It is shown here and will be skipped in the next analysis examples.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"HHY_gesZdrM1"},"source":["---\n","\n","## 2Ô∏è‚É£ Entity analysis\n","\n","Entity analysis inspects the given text for known entities (proper nouns such as public figures, landmarks, etc.), and returns information about those entities. It is performed with the `analyze_entities` method which returns an `AnalyzeEntitiesResponse`.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6NOJl4LQdrM8"},"outputs":[],"source":["def analyze_text_entities(text: str) -> language.AnalyzeEntitiesResponse:\n","    client = language.LanguageServiceClient()\n","    document = language.Document(\n","        content=text,\n","        type_=language.Document.Type.PLAIN_TEXT,\n","    )\n","    return client.analyze_entities(document=document)\n","\n","\n","def show_text_entities(response: language.AnalyzeEntitiesResponse):\n","    columns = (\"name\", \"type\", \"salience\", \"mid\", \"wikipedia_url\")\n","    data = (\n","        (\n","            entity.name,\n","            entity.type_.name,\n","            entity.salience,\n","            entity.metadata.get(\"mid\", \"\"),\n","            entity.metadata.get(\"wikipedia_url\", \"\"),\n","        )\n","        for entity in response.entities\n","    )\n","    formats = {\"salience\": \"{:.1%}\"}\n","    show_table(columns, data, formats)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"tdiVhvLrxJx7"},"source":["Now, let's perform an analysis:\n"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":269},"executionInfo":{"elapsed":472,"status":"ok","timestamp":1686139232355,"user":{"displayName":"Laurent Picard (GCP Demos)","userId":"15587849602996756367"},"user_tz":-120},"id":"28jLkpSqZ5Fc","outputId":"b232fa47-f142-4a02-e033-8a892ea5fa56"},"outputs":[{"data":{"text/html":["<style type=\"text/css\">\n","#T_540c5 th {\n","  text-align: center;\n","}\n","#T_540c5_row0_col0, #T_540c5_row0_col1, #T_540c5_row0_col3, #T_540c5_row0_col4, #T_540c5_row1_col0, #T_540c5_row1_col1, #T_540c5_row1_col3, #T_540c5_row1_col4, #T_540c5_row2_col0, #T_540c5_row2_col1, #T_540c5_row2_col3, #T_540c5_row2_col4, #T_540c5_row3_col0, #T_540c5_row3_col1, #T_540c5_row3_col3, #T_540c5_row3_col4, #T_540c5_row4_col0, #T_540c5_row4_col1, #T_540c5_row4_col3, #T_540c5_row4_col4, #T_540c5_row5_col0, #T_540c5_row5_col1, #T_540c5_row5_col3, #T_540c5_row5_col4, #T_540c5_row6_col0, #T_540c5_row6_col1, #T_540c5_row6_col3, #T_540c5_row6_col4 {\n","  text-align: left;\n","}\n","</style>\n","<table id=\"T_540c5\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th id=\"T_540c5_level0_col0\" class=\"col_heading level0 col0\" >name</th>\n","      <th id=\"T_540c5_level0_col1\" class=\"col_heading level0 col1\" >type</th>\n","      <th id=\"T_540c5_level0_col2\" class=\"col_heading level0 col2\" >salience</th>\n","      <th id=\"T_540c5_level0_col3\" class=\"col_heading level0 col3\" >mid</th>\n","      <th id=\"T_540c5_level0_col4\" class=\"col_heading level0 col4\" >wikipedia_url</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td id=\"T_540c5_row0_col0\" class=\"data row0 col0\" >Guido van Rossum</td>\n","      <td id=\"T_540c5_row0_col1\" class=\"data row0 col1\" >PERSON</td>\n","      <td id=\"T_540c5_row0_col2\" class=\"data row0 col2\" >49.8%</td>\n","      <td id=\"T_540c5_row0_col3\" class=\"data row0 col3\" >/m/01h05c</td>\n","      <td id=\"T_540c5_row0_col4\" class=\"data row0 col4\" >https://en.wikipedia.org/wiki/Guido_van_Rossum</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_540c5_row1_col0\" class=\"data row1 col0\" >Python</td>\n","      <td id=\"T_540c5_row1_col1\" class=\"data row1 col1\" >ORGANIZATION</td>\n","      <td id=\"T_540c5_row1_col2\" class=\"data row1 col2\" >38.4%</td>\n","      <td id=\"T_540c5_row1_col3\" class=\"data row1 col3\" >/m/05z1_</td>\n","      <td id=\"T_540c5_row1_col4\" class=\"data row1 col4\" >https://en.wikipedia.org/wiki/Python_(programming_language)</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_540c5_row2_col0\" class=\"data row2 col0\" >creator</td>\n","      <td id=\"T_540c5_row2_col1\" class=\"data row2 col1\" >PERSON</td>\n","      <td id=\"T_540c5_row2_col2\" class=\"data row2 col2\" >5.1%</td>\n","      <td id=\"T_540c5_row2_col3\" class=\"data row2 col3\" ></td>\n","      <td id=\"T_540c5_row2_col4\" class=\"data row2 col4\" ></td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_540c5_row3_col0\" class=\"data row3 col0\" >Monty Python</td>\n","      <td id=\"T_540c5_row3_col1\" class=\"data row3 col1\" >PERSON</td>\n","      <td id=\"T_540c5_row3_col2\" class=\"data row3 col2\" >3.2%</td>\n","      <td id=\"T_540c5_row3_col3\" class=\"data row3 col3\" >/m/04sd0</td>\n","      <td id=\"T_540c5_row3_col4\" class=\"data row3 col4\" >https://en.wikipedia.org/wiki/Monty_Python</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_540c5_row4_col0\" class=\"data row4 col0\" >comedy troupe</td>\n","      <td id=\"T_540c5_row4_col1\" class=\"data row4 col1\" >PERSON</td>\n","      <td id=\"T_540c5_row4_col2\" class=\"data row4 col2\" >1.6%</td>\n","      <td id=\"T_540c5_row4_col3\" class=\"data row4 col3\" ></td>\n","      <td id=\"T_540c5_row4_col4\" class=\"data row4 col4\" ></td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_540c5_row5_col0\" class=\"data row5 col0\" >Haarlem</td>\n","      <td id=\"T_540c5_row5_col1\" class=\"data row5 col1\" >LOCATION</td>\n","      <td id=\"T_540c5_row5_col2\" class=\"data row5 col2\" >1.0%</td>\n","      <td id=\"T_540c5_row5_col3\" class=\"data row5 col3\" >/m/0h095</td>\n","      <td id=\"T_540c5_row5_col4\" class=\"data row5 col4\" >https://en.wikipedia.org/wiki/Haarlem</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_540c5_row6_col0\" class=\"data row6 col0\" >Netherlands</td>\n","      <td id=\"T_540c5_row6_col1\" class=\"data row6 col1\" >LOCATION</td>\n","      <td id=\"T_540c5_row6_col2\" class=\"data row6 col2\" >0.7%</td>\n","      <td id=\"T_540c5_row6_col3\" class=\"data row6 col3\" >/m/059j2</td>\n","      <td id=\"T_540c5_row6_col4\" class=\"data row6 col4\" >https://en.wikipedia.org/wiki/Netherlands</td>\n","    </tr>\n","  </tbody>\n","</table>\n"],"text/plain":["<pandas.io.formats.style.Styler at 0x7f25ee60bdf0>"]},"metadata":{},"output_type":"display_data"}],"source":["# Input\n","text = \"Guido van Rossum is best known as the creator of Python, which he named after the Monty Python comedy troupe. He was born in Haarlem, Netherlands.\"  # @param {type:\"string\"}\n","\n","# Send a request to the API\n","analyze_entities_response = analyze_text_entities(text)\n","\n","# Show the results\n","show_text_entities(analyze_entities_response)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ByF1lee3vCIE"},"source":["Notes:\n","\n","- For information on which languages are supported by this method, see [Language Support](https://cloud.google.com/natural-language/docs/languages#entity_analysis).\n","- The `type` of the entity is an enum that lets you classify or differentiate entities. For example, this can help distinguish the similarly named entities _‚ÄúT.E. Lawrence‚Äù_ (a `PERSON`) from _‚ÄúLawrence of Arabia‚Äù_ (the film) (tagged as a `WORK_OF_ART`). See [`Entity.Type`](https://cloud.google.com/python/docs/reference/language/latest/google.cloud.language_v1.types.Entity.Type).\n","- The entity `salience` indicates the importance or relevance of this entity to the entire document text. This score can assist information retrieval and summarization by prioritizing salient entities. Scores closer to 0.0 are less important, while scores closer to 1.0 are highly important.\n","- For more information, see [Entity analysis](https://cloud.google.com/natural-language/docs/basics#entity_analysis).\n","- You can also combine both entity analysis and sentiment analysis with the `analyze_entity_sentiment` method. See [Entity sentiment analysis](https://cloud.google.com/natural-language/docs/basics#entity_analysis).\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"uqzc9quwdurW"},"source":["---\n","\n","## 3Ô∏è‚É£ Syntax analysis\n","\n","Syntax analysis extracts linguistic information, breaking up the given text into a series of sentences and tokens (generally based on word boundaries), providing further analysis on those tokens. It is performed with the `analyze_syntax` method which returns an `AnalyzeSyntaxResponse`.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iAOfoJEUdurW"},"outputs":[],"source":["def analyze_text_syntax(text: str) -> language.AnalyzeSyntaxResponse:\n","    client = language.LanguageServiceClient()\n","    document = language.Document(\n","        content=text,\n","        type_=language.Document.Type.PLAIN_TEXT,\n","    )\n","    return client.analyze_syntax(document=document)\n","\n","\n","def get_token_info(token: language.Token | None) -> list[str]:\n","    parts = [\n","        \"tag\",\n","        \"aspect\",\n","        \"case\",\n","        \"form\",\n","        \"gender\",\n","        \"mood\",\n","        \"number\",\n","        \"person\",\n","        \"proper\",\n","        \"reciprocity\",\n","        \"tense\",\n","        \"voice\",\n","    ]\n","    if not token:\n","        return [\"token\", \"lemma\"] + parts\n","\n","    text = token.text.content\n","    lemma = token.lemma if token.lemma != token.text.content else \"\"\n","    info = [text, lemma]\n","    for part in parts:\n","        pos = token.part_of_speech\n","        info.append(getattr(pos, part).name if part in pos else \"\")\n","\n","    return info\n","\n","\n","def show_text_syntax(response: language.AnalyzeSyntaxResponse):\n","    tokens = len(response.tokens)\n","    sentences = len(response.sentences)\n","    columns = get_token_info(None)\n","    data = (get_token_info(token) for token in response.tokens)\n","\n","    print(f\"Analyzed {tokens} token(s) from {sentences} sentence(s)\")\n","    show_table(columns, data, remove_empty_columns=True)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"YSDS0-o0xNqf"},"source":["Now, let's perform an analysis:\n"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":693},"executionInfo":{"elapsed":651,"status":"ok","timestamp":1686139238417,"user":{"displayName":"Laurent Picard (GCP Demos)","userId":"15587849602996756367"},"user_tz":-120},"id":"spjtSAfaDqQL","outputId":"f4cc7dde-d842-45ba-ec4c-85df6996d628"},"outputs":[{"name":"stdout","output_type":"stream","text":["Analyzed 20 token(s) from 2 sentence(s)\n"]},{"data":{"text/html":["<style type=\"text/css\">\n","#T_03bde th {\n","  text-align: center;\n","}\n","#T_03bde_row0_col0, #T_03bde_row0_col1, #T_03bde_row0_col2, #T_03bde_row0_col3, #T_03bde_row0_col4, #T_03bde_row0_col5, #T_03bde_row0_col6, #T_03bde_row0_col7, #T_03bde_row0_col8, #T_03bde_row0_col9, #T_03bde_row0_col10, #T_03bde_row1_col0, #T_03bde_row1_col1, #T_03bde_row1_col2, #T_03bde_row1_col3, #T_03bde_row1_col4, #T_03bde_row1_col5, #T_03bde_row1_col6, #T_03bde_row1_col7, #T_03bde_row1_col8, #T_03bde_row1_col9, #T_03bde_row1_col10, #T_03bde_row2_col0, #T_03bde_row2_col1, #T_03bde_row2_col2, #T_03bde_row2_col3, #T_03bde_row2_col4, #T_03bde_row2_col5, #T_03bde_row2_col6, #T_03bde_row2_col7, #T_03bde_row2_col8, #T_03bde_row2_col9, #T_03bde_row2_col10, #T_03bde_row3_col0, #T_03bde_row3_col1, #T_03bde_row3_col2, #T_03bde_row3_col3, #T_03bde_row3_col4, #T_03bde_row3_col5, #T_03bde_row3_col6, #T_03bde_row3_col7, #T_03bde_row3_col8, #T_03bde_row3_col9, #T_03bde_row3_col10, #T_03bde_row4_col0, #T_03bde_row4_col1, #T_03bde_row4_col2, #T_03bde_row4_col3, #T_03bde_row4_col4, #T_03bde_row4_col5, #T_03bde_row4_col6, #T_03bde_row4_col7, #T_03bde_row4_col8, #T_03bde_row4_col9, #T_03bde_row4_col10, #T_03bde_row5_col0, #T_03bde_row5_col1, #T_03bde_row5_col2, #T_03bde_row5_col3, #T_03bde_row5_col4, #T_03bde_row5_col5, #T_03bde_row5_col6, #T_03bde_row5_col7, #T_03bde_row5_col8, #T_03bde_row5_col9, #T_03bde_row5_col10, #T_03bde_row6_col0, #T_03bde_row6_col1, #T_03bde_row6_col2, #T_03bde_row6_col3, #T_03bde_row6_col4, #T_03bde_row6_col5, #T_03bde_row6_col6, #T_03bde_row6_col7, #T_03bde_row6_col8, #T_03bde_row6_col9, #T_03bde_row6_col10, #T_03bde_row7_col0, #T_03bde_row7_col1, #T_03bde_row7_col2, #T_03bde_row7_col3, #T_03bde_row7_col4, #T_03bde_row7_col5, #T_03bde_row7_col6, #T_03bde_row7_col7, #T_03bde_row7_col8, #T_03bde_row7_col9, #T_03bde_row7_col10, #T_03bde_row8_col0, #T_03bde_row8_col1, #T_03bde_row8_col2, #T_03bde_row8_col3, #T_03bde_row8_col4, #T_03bde_row8_col5, #T_03bde_row8_col6, #T_03bde_row8_col7, #T_03bde_row8_col8, #T_03bde_row8_col9, #T_03bde_row8_col10, #T_03bde_row9_col0, #T_03bde_row9_col1, #T_03bde_row9_col2, #T_03bde_row9_col3, #T_03bde_row9_col4, #T_03bde_row9_col5, #T_03bde_row9_col6, #T_03bde_row9_col7, #T_03bde_row9_col8, #T_03bde_row9_col9, #T_03bde_row9_col10, #T_03bde_row10_col0, #T_03bde_row10_col1, #T_03bde_row10_col2, #T_03bde_row10_col3, #T_03bde_row10_col4, #T_03bde_row10_col5, #T_03bde_row10_col6, #T_03bde_row10_col7, #T_03bde_row10_col8, #T_03bde_row10_col9, #T_03bde_row10_col10, #T_03bde_row11_col0, #T_03bde_row11_col1, #T_03bde_row11_col2, #T_03bde_row11_col3, #T_03bde_row11_col4, #T_03bde_row11_col5, #T_03bde_row11_col6, #T_03bde_row11_col7, #T_03bde_row11_col8, #T_03bde_row11_col9, #T_03bde_row11_col10, #T_03bde_row12_col0, #T_03bde_row12_col1, #T_03bde_row12_col2, #T_03bde_row12_col3, #T_03bde_row12_col4, #T_03bde_row12_col5, #T_03bde_row12_col6, #T_03bde_row12_col7, #T_03bde_row12_col8, #T_03bde_row12_col9, #T_03bde_row12_col10, #T_03bde_row13_col0, #T_03bde_row13_col1, #T_03bde_row13_col2, #T_03bde_row13_col3, #T_03bde_row13_col4, #T_03bde_row13_col5, #T_03bde_row13_col6, #T_03bde_row13_col7, #T_03bde_row13_col8, #T_03bde_row13_col9, #T_03bde_row13_col10, #T_03bde_row14_col0, #T_03bde_row14_col1, #T_03bde_row14_col2, #T_03bde_row14_col3, #T_03bde_row14_col4, #T_03bde_row14_col5, #T_03bde_row14_col6, #T_03bde_row14_col7, #T_03bde_row14_col8, #T_03bde_row14_col9, #T_03bde_row14_col10, #T_03bde_row15_col0, #T_03bde_row15_col1, #T_03bde_row15_col2, #T_03bde_row15_col3, #T_03bde_row15_col4, #T_03bde_row15_col5, #T_03bde_row15_col6, #T_03bde_row15_col7, #T_03bde_row15_col8, #T_03bde_row15_col9, #T_03bde_row15_col10, #T_03bde_row16_col0, #T_03bde_row16_col1, #T_03bde_row16_col2, #T_03bde_row16_col3, #T_03bde_row16_col4, #T_03bde_row16_col5, #T_03bde_row16_col6, #T_03bde_row16_col7, #T_03bde_row16_col8, #T_03bde_row16_col9, #T_03bde_row16_col10, #T_03bde_row17_col0, #T_03bde_row17_col1, #T_03bde_row17_col2, #T_03bde_row17_col3, #T_03bde_row17_col4, #T_03bde_row17_col5, #T_03bde_row17_col6, #T_03bde_row17_col7, #T_03bde_row17_col8, #T_03bde_row17_col9, #T_03bde_row17_col10, #T_03bde_row18_col0, #T_03bde_row18_col1, #T_03bde_row18_col2, #T_03bde_row18_col3, #T_03bde_row18_col4, #T_03bde_row18_col5, #T_03bde_row18_col6, #T_03bde_row18_col7, #T_03bde_row18_col8, #T_03bde_row18_col9, #T_03bde_row18_col10, #T_03bde_row19_col0, #T_03bde_row19_col1, #T_03bde_row19_col2, #T_03bde_row19_col3, #T_03bde_row19_col4, #T_03bde_row19_col5, #T_03bde_row19_col6, #T_03bde_row19_col7, #T_03bde_row19_col8, #T_03bde_row19_col9, #T_03bde_row19_col10 {\n","  text-align: left;\n","}\n","</style>\n","<table id=\"T_03bde\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th id=\"T_03bde_level0_col0\" class=\"col_heading level0 col0\" >token</th>\n","      <th id=\"T_03bde_level0_col1\" class=\"col_heading level0 col1\" >lemma</th>\n","      <th id=\"T_03bde_level0_col2\" class=\"col_heading level0 col2\" >tag</th>\n","      <th id=\"T_03bde_level0_col3\" class=\"col_heading level0 col3\" >case</th>\n","      <th id=\"T_03bde_level0_col4\" class=\"col_heading level0 col4\" >gender</th>\n","      <th id=\"T_03bde_level0_col5\" class=\"col_heading level0 col5\" >mood</th>\n","      <th id=\"T_03bde_level0_col6\" class=\"col_heading level0 col6\" >number</th>\n","      <th id=\"T_03bde_level0_col7\" class=\"col_heading level0 col7\" >person</th>\n","      <th id=\"T_03bde_level0_col8\" class=\"col_heading level0 col8\" >proper</th>\n","      <th id=\"T_03bde_level0_col9\" class=\"col_heading level0 col9\" >tense</th>\n","      <th id=\"T_03bde_level0_col10\" class=\"col_heading level0 col10\" >voice</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td id=\"T_03bde_row0_col0\" class=\"data row0 col0\" >Guido</td>\n","      <td id=\"T_03bde_row0_col1\" class=\"data row0 col1\" ></td>\n","      <td id=\"T_03bde_row0_col2\" class=\"data row0 col2\" >NOUN</td>\n","      <td id=\"T_03bde_row0_col3\" class=\"data row0 col3\" ></td>\n","      <td id=\"T_03bde_row0_col4\" class=\"data row0 col4\" ></td>\n","      <td id=\"T_03bde_row0_col5\" class=\"data row0 col5\" ></td>\n","      <td id=\"T_03bde_row0_col6\" class=\"data row0 col6\" >SINGULAR</td>\n","      <td id=\"T_03bde_row0_col7\" class=\"data row0 col7\" ></td>\n","      <td id=\"T_03bde_row0_col8\" class=\"data row0 col8\" >PROPER</td>\n","      <td id=\"T_03bde_row0_col9\" class=\"data row0 col9\" ></td>\n","      <td id=\"T_03bde_row0_col10\" class=\"data row0 col10\" ></td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_03bde_row1_col0\" class=\"data row1 col0\" >van</td>\n","      <td id=\"T_03bde_row1_col1\" class=\"data row1 col1\" ></td>\n","      <td id=\"T_03bde_row1_col2\" class=\"data row1 col2\" >NOUN</td>\n","      <td id=\"T_03bde_row1_col3\" class=\"data row1 col3\" ></td>\n","      <td id=\"T_03bde_row1_col4\" class=\"data row1 col4\" ></td>\n","      <td id=\"T_03bde_row1_col5\" class=\"data row1 col5\" ></td>\n","      <td id=\"T_03bde_row1_col6\" class=\"data row1 col6\" >SINGULAR</td>\n","      <td id=\"T_03bde_row1_col7\" class=\"data row1 col7\" ></td>\n","      <td id=\"T_03bde_row1_col8\" class=\"data row1 col8\" >PROPER</td>\n","      <td id=\"T_03bde_row1_col9\" class=\"data row1 col9\" ></td>\n","      <td id=\"T_03bde_row1_col10\" class=\"data row1 col10\" ></td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_03bde_row2_col0\" class=\"data row2 col0\" >Rossum</td>\n","      <td id=\"T_03bde_row2_col1\" class=\"data row2 col1\" ></td>\n","      <td id=\"T_03bde_row2_col2\" class=\"data row2 col2\" >NOUN</td>\n","      <td id=\"T_03bde_row2_col3\" class=\"data row2 col3\" ></td>\n","      <td id=\"T_03bde_row2_col4\" class=\"data row2 col4\" ></td>\n","      <td id=\"T_03bde_row2_col5\" class=\"data row2 col5\" ></td>\n","      <td id=\"T_03bde_row2_col6\" class=\"data row2 col6\" >SINGULAR</td>\n","      <td id=\"T_03bde_row2_col7\" class=\"data row2 col7\" ></td>\n","      <td id=\"T_03bde_row2_col8\" class=\"data row2 col8\" >PROPER</td>\n","      <td id=\"T_03bde_row2_col9\" class=\"data row2 col9\" ></td>\n","      <td id=\"T_03bde_row2_col10\" class=\"data row2 col10\" ></td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_03bde_row3_col0\" class=\"data row3 col0\" >is</td>\n","      <td id=\"T_03bde_row3_col1\" class=\"data row3 col1\" >be</td>\n","      <td id=\"T_03bde_row3_col2\" class=\"data row3 col2\" >VERB</td>\n","      <td id=\"T_03bde_row3_col3\" class=\"data row3 col3\" ></td>\n","      <td id=\"T_03bde_row3_col4\" class=\"data row3 col4\" ></td>\n","      <td id=\"T_03bde_row3_col5\" class=\"data row3 col5\" >INDICATIVE</td>\n","      <td id=\"T_03bde_row3_col6\" class=\"data row3 col6\" >SINGULAR</td>\n","      <td id=\"T_03bde_row3_col7\" class=\"data row3 col7\" >THIRD</td>\n","      <td id=\"T_03bde_row3_col8\" class=\"data row3 col8\" ></td>\n","      <td id=\"T_03bde_row3_col9\" class=\"data row3 col9\" >PRESENT</td>\n","      <td id=\"T_03bde_row3_col10\" class=\"data row3 col10\" ></td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_03bde_row4_col0\" class=\"data row4 col0\" >best</td>\n","      <td id=\"T_03bde_row4_col1\" class=\"data row4 col1\" >well</td>\n","      <td id=\"T_03bde_row4_col2\" class=\"data row4 col2\" >ADV</td>\n","      <td id=\"T_03bde_row4_col3\" class=\"data row4 col3\" ></td>\n","      <td id=\"T_03bde_row4_col4\" class=\"data row4 col4\" ></td>\n","      <td id=\"T_03bde_row4_col5\" class=\"data row4 col5\" ></td>\n","      <td id=\"T_03bde_row4_col6\" class=\"data row4 col6\" ></td>\n","      <td id=\"T_03bde_row4_col7\" class=\"data row4 col7\" ></td>\n","      <td id=\"T_03bde_row4_col8\" class=\"data row4 col8\" ></td>\n","      <td id=\"T_03bde_row4_col9\" class=\"data row4 col9\" ></td>\n","      <td id=\"T_03bde_row4_col10\" class=\"data row4 col10\" ></td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_03bde_row5_col0\" class=\"data row5 col0\" >known</td>\n","      <td id=\"T_03bde_row5_col1\" class=\"data row5 col1\" >know</td>\n","      <td id=\"T_03bde_row5_col2\" class=\"data row5 col2\" >VERB</td>\n","      <td id=\"T_03bde_row5_col3\" class=\"data row5 col3\" ></td>\n","      <td id=\"T_03bde_row5_col4\" class=\"data row5 col4\" ></td>\n","      <td id=\"T_03bde_row5_col5\" class=\"data row5 col5\" ></td>\n","      <td id=\"T_03bde_row5_col6\" class=\"data row5 col6\" ></td>\n","      <td id=\"T_03bde_row5_col7\" class=\"data row5 col7\" ></td>\n","      <td id=\"T_03bde_row5_col8\" class=\"data row5 col8\" ></td>\n","      <td id=\"T_03bde_row5_col9\" class=\"data row5 col9\" >PAST</td>\n","      <td id=\"T_03bde_row5_col10\" class=\"data row5 col10\" ></td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_03bde_row6_col0\" class=\"data row6 col0\" >as</td>\n","      <td id=\"T_03bde_row6_col1\" class=\"data row6 col1\" ></td>\n","      <td id=\"T_03bde_row6_col2\" class=\"data row6 col2\" >ADP</td>\n","      <td id=\"T_03bde_row6_col3\" class=\"data row6 col3\" ></td>\n","      <td id=\"T_03bde_row6_col4\" class=\"data row6 col4\" ></td>\n","      <td id=\"T_03bde_row6_col5\" class=\"data row6 col5\" ></td>\n","      <td id=\"T_03bde_row6_col6\" class=\"data row6 col6\" ></td>\n","      <td id=\"T_03bde_row6_col7\" class=\"data row6 col7\" ></td>\n","      <td id=\"T_03bde_row6_col8\" class=\"data row6 col8\" ></td>\n","      <td id=\"T_03bde_row6_col9\" class=\"data row6 col9\" ></td>\n","      <td id=\"T_03bde_row6_col10\" class=\"data row6 col10\" ></td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_03bde_row7_col0\" class=\"data row7 col0\" >the</td>\n","      <td id=\"T_03bde_row7_col1\" class=\"data row7 col1\" ></td>\n","      <td id=\"T_03bde_row7_col2\" class=\"data row7 col2\" >DET</td>\n","      <td id=\"T_03bde_row7_col3\" class=\"data row7 col3\" ></td>\n","      <td id=\"T_03bde_row7_col4\" class=\"data row7 col4\" ></td>\n","      <td id=\"T_03bde_row7_col5\" class=\"data row7 col5\" ></td>\n","      <td id=\"T_03bde_row7_col6\" class=\"data row7 col6\" ></td>\n","      <td id=\"T_03bde_row7_col7\" class=\"data row7 col7\" ></td>\n","      <td id=\"T_03bde_row7_col8\" class=\"data row7 col8\" ></td>\n","      <td id=\"T_03bde_row7_col9\" class=\"data row7 col9\" ></td>\n","      <td id=\"T_03bde_row7_col10\" class=\"data row7 col10\" ></td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_03bde_row8_col0\" class=\"data row8 col0\" >creator</td>\n","      <td id=\"T_03bde_row8_col1\" class=\"data row8 col1\" ></td>\n","      <td id=\"T_03bde_row8_col2\" class=\"data row8 col2\" >NOUN</td>\n","      <td id=\"T_03bde_row8_col3\" class=\"data row8 col3\" ></td>\n","      <td id=\"T_03bde_row8_col4\" class=\"data row8 col4\" ></td>\n","      <td id=\"T_03bde_row8_col5\" class=\"data row8 col5\" ></td>\n","      <td id=\"T_03bde_row8_col6\" class=\"data row8 col6\" >SINGULAR</td>\n","      <td id=\"T_03bde_row8_col7\" class=\"data row8 col7\" ></td>\n","      <td id=\"T_03bde_row8_col8\" class=\"data row8 col8\" ></td>\n","      <td id=\"T_03bde_row8_col9\" class=\"data row8 col9\" ></td>\n","      <td id=\"T_03bde_row8_col10\" class=\"data row8 col10\" ></td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_03bde_row9_col0\" class=\"data row9 col0\" >of</td>\n","      <td id=\"T_03bde_row9_col1\" class=\"data row9 col1\" ></td>\n","      <td id=\"T_03bde_row9_col2\" class=\"data row9 col2\" >ADP</td>\n","      <td id=\"T_03bde_row9_col3\" class=\"data row9 col3\" ></td>\n","      <td id=\"T_03bde_row9_col4\" class=\"data row9 col4\" ></td>\n","      <td id=\"T_03bde_row9_col5\" class=\"data row9 col5\" ></td>\n","      <td id=\"T_03bde_row9_col6\" class=\"data row9 col6\" ></td>\n","      <td id=\"T_03bde_row9_col7\" class=\"data row9 col7\" ></td>\n","      <td id=\"T_03bde_row9_col8\" class=\"data row9 col8\" ></td>\n","      <td id=\"T_03bde_row9_col9\" class=\"data row9 col9\" ></td>\n","      <td id=\"T_03bde_row9_col10\" class=\"data row9 col10\" ></td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_03bde_row10_col0\" class=\"data row10 col0\" >Python</td>\n","      <td id=\"T_03bde_row10_col1\" class=\"data row10 col1\" ></td>\n","      <td id=\"T_03bde_row10_col2\" class=\"data row10 col2\" >NOUN</td>\n","      <td id=\"T_03bde_row10_col3\" class=\"data row10 col3\" ></td>\n","      <td id=\"T_03bde_row10_col4\" class=\"data row10 col4\" ></td>\n","      <td id=\"T_03bde_row10_col5\" class=\"data row10 col5\" ></td>\n","      <td id=\"T_03bde_row10_col6\" class=\"data row10 col6\" >SINGULAR</td>\n","      <td id=\"T_03bde_row10_col7\" class=\"data row10 col7\" ></td>\n","      <td id=\"T_03bde_row10_col8\" class=\"data row10 col8\" >PROPER</td>\n","      <td id=\"T_03bde_row10_col9\" class=\"data row10 col9\" ></td>\n","      <td id=\"T_03bde_row10_col10\" class=\"data row10 col10\" ></td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_03bde_row11_col0\" class=\"data row11 col0\" >.</td>\n","      <td id=\"T_03bde_row11_col1\" class=\"data row11 col1\" ></td>\n","      <td id=\"T_03bde_row11_col2\" class=\"data row11 col2\" >PUNCT</td>\n","      <td id=\"T_03bde_row11_col3\" class=\"data row11 col3\" ></td>\n","      <td id=\"T_03bde_row11_col4\" class=\"data row11 col4\" ></td>\n","      <td id=\"T_03bde_row11_col5\" class=\"data row11 col5\" ></td>\n","      <td id=\"T_03bde_row11_col6\" class=\"data row11 col6\" ></td>\n","      <td id=\"T_03bde_row11_col7\" class=\"data row11 col7\" ></td>\n","      <td id=\"T_03bde_row11_col8\" class=\"data row11 col8\" ></td>\n","      <td id=\"T_03bde_row11_col9\" class=\"data row11 col9\" ></td>\n","      <td id=\"T_03bde_row11_col10\" class=\"data row11 col10\" ></td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_03bde_row12_col0\" class=\"data row12 col0\" >He</td>\n","      <td id=\"T_03bde_row12_col1\" class=\"data row12 col1\" ></td>\n","      <td id=\"T_03bde_row12_col2\" class=\"data row12 col2\" >PRON</td>\n","      <td id=\"T_03bde_row12_col3\" class=\"data row12 col3\" >NOMINATIVE</td>\n","      <td id=\"T_03bde_row12_col4\" class=\"data row12 col4\" >MASCULINE</td>\n","      <td id=\"T_03bde_row12_col5\" class=\"data row12 col5\" ></td>\n","      <td id=\"T_03bde_row12_col6\" class=\"data row12 col6\" >SINGULAR</td>\n","      <td id=\"T_03bde_row12_col7\" class=\"data row12 col7\" >THIRD</td>\n","      <td id=\"T_03bde_row12_col8\" class=\"data row12 col8\" ></td>\n","      <td id=\"T_03bde_row12_col9\" class=\"data row12 col9\" ></td>\n","      <td id=\"T_03bde_row12_col10\" class=\"data row12 col10\" ></td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_03bde_row13_col0\" class=\"data row13 col0\" >was</td>\n","      <td id=\"T_03bde_row13_col1\" class=\"data row13 col1\" >be</td>\n","      <td id=\"T_03bde_row13_col2\" class=\"data row13 col2\" >VERB</td>\n","      <td id=\"T_03bde_row13_col3\" class=\"data row13 col3\" ></td>\n","      <td id=\"T_03bde_row13_col4\" class=\"data row13 col4\" ></td>\n","      <td id=\"T_03bde_row13_col5\" class=\"data row13 col5\" >INDICATIVE</td>\n","      <td id=\"T_03bde_row13_col6\" class=\"data row13 col6\" >SINGULAR</td>\n","      <td id=\"T_03bde_row13_col7\" class=\"data row13 col7\" >THIRD</td>\n","      <td id=\"T_03bde_row13_col8\" class=\"data row13 col8\" ></td>\n","      <td id=\"T_03bde_row13_col9\" class=\"data row13 col9\" >PAST</td>\n","      <td id=\"T_03bde_row13_col10\" class=\"data row13 col10\" ></td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_03bde_row14_col0\" class=\"data row14 col0\" >born</td>\n","      <td id=\"T_03bde_row14_col1\" class=\"data row14 col1\" >bear</td>\n","      <td id=\"T_03bde_row14_col2\" class=\"data row14 col2\" >VERB</td>\n","      <td id=\"T_03bde_row14_col3\" class=\"data row14 col3\" ></td>\n","      <td id=\"T_03bde_row14_col4\" class=\"data row14 col4\" ></td>\n","      <td id=\"T_03bde_row14_col5\" class=\"data row14 col5\" ></td>\n","      <td id=\"T_03bde_row14_col6\" class=\"data row14 col6\" ></td>\n","      <td id=\"T_03bde_row14_col7\" class=\"data row14 col7\" ></td>\n","      <td id=\"T_03bde_row14_col8\" class=\"data row14 col8\" ></td>\n","      <td id=\"T_03bde_row14_col9\" class=\"data row14 col9\" >PAST</td>\n","      <td id=\"T_03bde_row14_col10\" class=\"data row14 col10\" >PASSIVE</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_03bde_row15_col0\" class=\"data row15 col0\" >in</td>\n","      <td id=\"T_03bde_row15_col1\" class=\"data row15 col1\" ></td>\n","      <td id=\"T_03bde_row15_col2\" class=\"data row15 col2\" >ADP</td>\n","      <td id=\"T_03bde_row15_col3\" class=\"data row15 col3\" ></td>\n","      <td id=\"T_03bde_row15_col4\" class=\"data row15 col4\" ></td>\n","      <td id=\"T_03bde_row15_col5\" class=\"data row15 col5\" ></td>\n","      <td id=\"T_03bde_row15_col6\" class=\"data row15 col6\" ></td>\n","      <td id=\"T_03bde_row15_col7\" class=\"data row15 col7\" ></td>\n","      <td id=\"T_03bde_row15_col8\" class=\"data row15 col8\" ></td>\n","      <td id=\"T_03bde_row15_col9\" class=\"data row15 col9\" ></td>\n","      <td id=\"T_03bde_row15_col10\" class=\"data row15 col10\" ></td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_03bde_row16_col0\" class=\"data row16 col0\" >Haarlem</td>\n","      <td id=\"T_03bde_row16_col1\" class=\"data row16 col1\" ></td>\n","      <td id=\"T_03bde_row16_col2\" class=\"data row16 col2\" >NOUN</td>\n","      <td id=\"T_03bde_row16_col3\" class=\"data row16 col3\" ></td>\n","      <td id=\"T_03bde_row16_col4\" class=\"data row16 col4\" ></td>\n","      <td id=\"T_03bde_row16_col5\" class=\"data row16 col5\" ></td>\n","      <td id=\"T_03bde_row16_col6\" class=\"data row16 col6\" >SINGULAR</td>\n","      <td id=\"T_03bde_row16_col7\" class=\"data row16 col7\" ></td>\n","      <td id=\"T_03bde_row16_col8\" class=\"data row16 col8\" >PROPER</td>\n","      <td id=\"T_03bde_row16_col9\" class=\"data row16 col9\" ></td>\n","      <td id=\"T_03bde_row16_col10\" class=\"data row16 col10\" ></td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_03bde_row17_col0\" class=\"data row17 col0\" >,</td>\n","      <td id=\"T_03bde_row17_col1\" class=\"data row17 col1\" ></td>\n","      <td id=\"T_03bde_row17_col2\" class=\"data row17 col2\" >PUNCT</td>\n","      <td id=\"T_03bde_row17_col3\" class=\"data row17 col3\" ></td>\n","      <td id=\"T_03bde_row17_col4\" class=\"data row17 col4\" ></td>\n","      <td id=\"T_03bde_row17_col5\" class=\"data row17 col5\" ></td>\n","      <td id=\"T_03bde_row17_col6\" class=\"data row17 col6\" ></td>\n","      <td id=\"T_03bde_row17_col7\" class=\"data row17 col7\" ></td>\n","      <td id=\"T_03bde_row17_col8\" class=\"data row17 col8\" ></td>\n","      <td id=\"T_03bde_row17_col9\" class=\"data row17 col9\" ></td>\n","      <td id=\"T_03bde_row17_col10\" class=\"data row17 col10\" ></td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_03bde_row18_col0\" class=\"data row18 col0\" >Netherlands</td>\n","      <td id=\"T_03bde_row18_col1\" class=\"data row18 col1\" ></td>\n","      <td id=\"T_03bde_row18_col2\" class=\"data row18 col2\" >NOUN</td>\n","      <td id=\"T_03bde_row18_col3\" class=\"data row18 col3\" ></td>\n","      <td id=\"T_03bde_row18_col4\" class=\"data row18 col4\" ></td>\n","      <td id=\"T_03bde_row18_col5\" class=\"data row18 col5\" ></td>\n","      <td id=\"T_03bde_row18_col6\" class=\"data row18 col6\" >SINGULAR</td>\n","      <td id=\"T_03bde_row18_col7\" class=\"data row18 col7\" ></td>\n","      <td id=\"T_03bde_row18_col8\" class=\"data row18 col8\" >PROPER</td>\n","      <td id=\"T_03bde_row18_col9\" class=\"data row18 col9\" ></td>\n","      <td id=\"T_03bde_row18_col10\" class=\"data row18 col10\" ></td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_03bde_row19_col0\" class=\"data row19 col0\" >.</td>\n","      <td id=\"T_03bde_row19_col1\" class=\"data row19 col1\" ></td>\n","      <td id=\"T_03bde_row19_col2\" class=\"data row19 col2\" >PUNCT</td>\n","      <td id=\"T_03bde_row19_col3\" class=\"data row19 col3\" ></td>\n","      <td id=\"T_03bde_row19_col4\" class=\"data row19 col4\" ></td>\n","      <td id=\"T_03bde_row19_col5\" class=\"data row19 col5\" ></td>\n","      <td id=\"T_03bde_row19_col6\" class=\"data row19 col6\" ></td>\n","      <td id=\"T_03bde_row19_col7\" class=\"data row19 col7\" ></td>\n","      <td id=\"T_03bde_row19_col8\" class=\"data row19 col8\" ></td>\n","      <td id=\"T_03bde_row19_col9\" class=\"data row19 col9\" ></td>\n","      <td id=\"T_03bde_row19_col10\" class=\"data row19 col10\" ></td>\n","    </tr>\n","  </tbody>\n","</table>\n"],"text/plain":["<pandas.io.formats.style.Styler at 0x7f25b7fc6e30>"]},"metadata":{},"output_type":"display_data"}],"source":["# Input\n","text = \"Guido van Rossum is best known as the creator of Python. He was born in Haarlem, Netherlands.\"  # @param {type:\"string\"}\n","\n","# Send a request to the API\n","analyze_syntax_response = analyze_text_syntax(text)\n","\n","# Show the results\n","show_text_syntax(analyze_syntax_response)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"VmRISErnz7iq"},"source":["There are multiple benefits to extracting the syntax information. One of them is to extract the lemmas. A `lemma` contains the \"root\" word upon which this token is based, which allows you to manage words with their canonical forms.\n","\n","If you dive deeper into the response insights, you'll also find the relationships between the tokens. Here is a visual interpretation showing the complete syntax analysis for this example:\n","\n","![Syntax Analysis](./pics/natural_language_syntax.png)\n","\n","> This is a screenshot from the online [Natural Language demo](https://cloud.google.com/natural-language/#natural-language-api-demo) with which you can create your own parse trees.\n","\n","For more information, see the following:\n","\n","- [`language.AnalyzeSyntaxResponse`](https://cloud.google.com/python/docs/reference/language/latest/google.cloud.language_v1.types.AnalyzeSyntaxResponse)\n","- [Language Support](https://cloud.google.com/natural-language/docs/languages#syntactic_analysis)\n","- [Syntactic analysis](https://cloud.google.com/natural-language/docs/basics#syntactic_analysis)\n","- [Morphology & Dependency Trees](https://cloud.google.com/natural-language/docs/morphology)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"KlDeFxQXdxHw"},"source":["---\n","\n","## 4Ô∏è‚É£ Content classification\n","\n","Content classification analyzes a document and returns a list of content categories that apply to the text found in the document. It is performed with the `classify_text` method which returns a `ClassifyTextResponse`.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q0dk8qaldxHw"},"outputs":[],"source":["def classify_text(text: str) -> language.ClassifyTextResponse:\n","    client = language.LanguageServiceClient()\n","    document = language.Document(\n","        content=text,\n","        type_=language.Document.Type.PLAIN_TEXT,\n","    )\n","    return client.classify_text(document=document)\n","\n","\n","def show_text_classification(response: language.ClassifyTextResponse):\n","    columns = [\"category\", \"confidence\"]\n","    data = ((category.name, category.confidence) for category in response.categories)\n","    formats = {\"confidence\": \"{:.0%}\"}\n","    show_table(columns, data, formats)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"1e6v4_D_xOpC"},"source":["Now, let's perform an analysis:\n"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":112},"executionInfo":{"elapsed":597,"status":"ok","timestamp":1686139243529,"user":{"displayName":"Laurent Picard (GCP Demos)","userId":"15587849602996756367"},"user_tz":-120},"id":"rX5uDw7gbqeW","outputId":"22de9958-95a1-4a8f-d2e8-2b3d1016d222"},"outputs":[{"data":{"text/html":["<style type=\"text/css\">\n","#T_d31e9 th {\n","  text-align: center;\n","}\n","#T_d31e9_row0_col0, #T_d31e9_row1_col0 {\n","  text-align: left;\n","}\n","</style>\n","<table id=\"T_d31e9\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th id=\"T_d31e9_level0_col0\" class=\"col_heading level0 col0\" >category</th>\n","      <th id=\"T_d31e9_level0_col1\" class=\"col_heading level0 col1\" >confidence</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td id=\"T_d31e9_row0_col0\" class=\"data row0 col0\" >/Computers & Electronics/Programming</td>\n","      <td id=\"T_d31e9_row0_col1\" class=\"data row0 col1\" >99%</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_d31e9_row1_col0\" class=\"data row1 col0\" >/Science/Computer Science</td>\n","      <td id=\"T_d31e9_row1_col1\" class=\"data row1 col1\" >99%</td>\n","    </tr>\n","  </tbody>\n","</table>\n"],"text/plain":["<pandas.io.formats.style.Styler at 0x7f25b7fc7760>"]},"metadata":{},"output_type":"display_data"}],"source":["# Input\n","text = \"Python is an interpreted, high-level, general-purpose programming language. Created by Guido van Rossum and first released in 1991, Python's design philosophy emphasizes code readability with its notable use of significant whitespace.\"  # @param {type:\"string\"}\n","\n","# Send a request to the API\n","classify_text_response = classify_text(text)\n","\n","# Show the results\n","show_text_classification(classify_text_response)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"lmTbxLTcU0DD"},"source":["> Important: You must supply a text block (document) with at least twenty tokens.\n","\n","For more information, see the following docs:\n","\n","- [`ClassifyTextResponse`](https://cloud.google.com/python/docs/reference/language/latest/google.cloud.language_v1.types.ClassifyTextResponse)\n","- [Language Support](https://cloud.google.com/natural-language/docs/languages#content_classification)\n","- [Content Classification](https://cloud.google.com/natural-language/docs/basics#content-classification)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"dNqzg9ylC0G2"},"source":["---\n","\n","## üéâ Congratulations\n","\n","You learned how to use the Natural Language API with Python!\n","\n","<center>\n","<table><tr><td>\n","<img src=\"pics/natural_language_api.png\" style=\"height:200px;\" height=\"200\" />\n","</td></tr></table>\n","<table><tr>\n"]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3.10.4 64-bit","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.4"}},"nbformat":4,"nbformat_minor":0}
