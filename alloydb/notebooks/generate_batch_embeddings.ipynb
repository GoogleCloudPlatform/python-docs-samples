{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "upi2EY4L9ei3"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbF2F2miAT4a"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/GoogleCloudPlatform/python-docs-samples/blob/main/alloydb/notebooks/generate_batch_embeddings.ipynb)\n",
        "\n",
        "---\n",
        "# Introduction\n",
        "\n",
        "This notebook shows you how to generate batch vector embeddings and store them in an AlloyDB database.\n",
        "\n",
        "With the steps listed here, you can dynamically build a batch of text chunks to embed based on character length of the source data in order to get more results per inference, leading to much more efficient embeddings generation. The process uses the psycopg library to efficiently load the embeddings into AlloyDB after they are generated. These techniques can significantly speed up the process of generating large batches of embeddings and storing them in AlloyDB vs using the native embedding() function (about 6.5x faster based on limited testing)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbcZUjT1yvTq"
      },
      "source": [
        "## What you'll need\n",
        "\n",
        "* A Google Cloud Account and Google Cloud Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHdR4fF3vLWA"
      },
      "source": [
        "# Setup and Requirements\n",
        "\n",
        "In the following instructions you will learn to:\n",
        "\n",
        "1. Install required dependencies for our application\n",
        "2. Set up authentication for our project\n",
        "3. Set up a AlloyDB for PostgreSQL Instance\n",
        "4. Import the data used by our application"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uy9KqgPQ4GBi"
      },
      "source": [
        "## Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_ppDxYf4Gqs",
        "outputId": "9ce1e63f-af4d-4676-acf6-8a55f9cb514c"
      },
      "outputs": [],
      "source": [
        "%pip install langchain-google-alloydb-pg==0.7.0 langchain==0.3.3 langchain-google-vertexai==2.0.4 google-cloud-alloydb-connector[pg8000]==1.4.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeUbHclxw7_l"
      },
      "source": [
        "## Authenticate to Google Cloud within Colab\n",
        "In order to access your Google Cloud Project from this notebook, you will need to Authenticate as an IAM user."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_Q9hyqdyEx6l"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCiNGP1Qxd6x"
      },
      "source": [
        "## Connect Your Google Cloud Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLUGlG6UE2CK",
        "outputId": "10f54259-bdb3-41af-a206-996e5c85a0b3"
      },
      "outputs": [],
      "source": [
        "# @markdown Please fill in the value below with your GCP project ID and then run the cell.\n",
        "\n",
        "# Please fill in these values.\n",
        "project_id = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "# Quick input validations.\n",
        "assert project_id, \"⚠️ Please provide a Google Cloud project ID\"\n",
        "\n",
        "# Configure gcloud.\n",
        "!gcloud config set project {project_id}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-oqMC5Ox-ZM"
      },
      "source": [
        "## Enable APIs for AlloyDB and Vertex AI within your project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-bzfFb4A-xK"
      },
      "source": [
        "You will need to enable these APIs in order to create an AlloyDB database and utilize Vertex AI as an embeddings service!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKWrwyfzyTwH",
        "outputId": "be612a6e-ec13-42d7-f8cf-18666a52b7d3"
      },
      "outputs": [],
      "source": [
        "# enable GCP services\n",
        "!gcloud services enable alloydb.googleapis.com aiplatform.googleapis.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gn8g7-wCyZU6"
      },
      "source": [
        "## Set up AlloyDB\n",
        "You will need a Postgres AlloyDB instance for the following stages of this notebook. Please set the following variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8q2lc-Po1mPv",
        "outputId": "a84eb397-6fcc-4c2e-ede8-fbbad9dc35e8"
      },
      "outputs": [],
      "source": [
        "# @markdown Please fill in the both the Google Cloud region and name of your AlloyDB instance. Once filled in, run the cell.\n",
        "\n",
        "# Please fill in these values.\n",
        "region = \"\"  # @param {type:\"string\"}\n",
        "cluster_name = \"\"  # @param {type:\"string\"}\n",
        "instance_name = \"\"  # @param {type:\"string\"}\n",
        "database_name = \"\"  # @param {type:\"string\"}\n",
        "password = input(\"Please provide a password to be used for 'postgres' database user: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T616pEOUygYQ"
      },
      "source": [
        "### Create an AlloyDB Instance\n",
        "If you have already created an AlloyDB Cluster and Instance, you can skip these steps and skip to the connectivity section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyZYX4Jo1vfh"
      },
      "source": [
        "> ⏳ - Creating an AlloyDB cluster may take a few minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "XXI1uUu3y8gc"
      },
      "outputs": [],
      "source": [
        "# Quick input validations.\n",
        "assert region, \"⚠️ Please provide a Google Cloud region\"\n",
        "assert instance_name, \"⚠️ Please provide the name of your instance\"\n",
        "assert database_name, \"⚠️ Please provide the name of your database_name\"\n",
        "\n",
        "# create the AlloyDB Cluster\n",
        "!gcloud beta alloydb clusters create {cluster_name} --password={password} --region={region}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8LkscYH5Vfp"
      },
      "source": [
        "Create an instance attached to our cluster with the following command.\n",
        "> ⏳ - Creating an AlloyDB instance may take a few minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "TkqQSWoY5Kab"
      },
      "outputs": [],
      "source": [
        "!gcloud beta alloydb instances create {instance_name} --instance-type=PRIMARY --cpu-count=2 --region={region} --cluster={cluster_name}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXsQ1UJv4ZVJ"
      },
      "source": [
        "To connect to your AlloyDB instance from this notebook, you will need to enable public IP on your instance. Alternatively, you can follow [these instructions](https://cloud.google.com/alloydb/docs/connect-external) to connect to an AlloyDB for PostgreSQL instance with Private IP from outside your VPC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "OPVWsQB04Yyl"
      },
      "outputs": [],
      "source": [
        "!gcloud beta alloydb instances update {instance_name} --region={region} --cluster={cluster_name} --assign-inbound-public-ip=ASSIGN_IPV4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjA6AiAzB2Du"
      },
      "source": [
        "Now create a connection pool to connect to our instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsLi0a7FIYjT"
      },
      "outputs": [],
      "source": [
        "from google.cloud.alloydb.connector import Connector, IPTypes\n",
        "import sqlalchemy\n",
        "\n",
        "\n",
        "connection_string = f\"projects/{project_id}/locations/{region}/clusters/{cluster_name}/instances/{instance_name}\"\n",
        "# initialize Connector object\n",
        "connector = Connector()\n",
        "\n",
        "\n",
        "# function to return the database connection\n",
        "def getconn():\n",
        "    conn = connector.connect(\n",
        "        connection_string,\n",
        "        \"pg8000\",\n",
        "        user=\"postgres\",\n",
        "        password=password,\n",
        "        db=\"postgres\",\n",
        "        enable_iam_auth=False,\n",
        "        ip_type=IPTypes.PUBLIC,\n",
        "    )\n",
        "    return conn\n",
        "\n",
        "\n",
        "# create connection pool\n",
        "pool = sqlalchemy.create_engine(\n",
        "    \"postgresql+pg8000://\", creator=getconn, isolation_level=\"AUTOCOMMIT\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_yNN1MnJpTR"
      },
      "source": [
        "### Create a Database\n",
        "\n",
        "Next you will create database to store the data for this application using the connection pool. Enabling public IP takes a few minutes, you may get an error that there is no public IP address. Please wait and retry this step if you hit an error!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "hPE6tt5eJqhq"
      },
      "outputs": [],
      "source": [
        "with pool.connect() as db_conn:\n",
        "    db_conn.execute(sqlalchemy.text(f\"CREATE DATABASE {database_name}\"))\n",
        "connector.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_K86id-dcjcm"
      },
      "source": [
        "#### Connect to our New Database\n",
        "\n",
        "Now you will add in a connection function that connects to your new database!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "fYKVQzv2cjcm"
      },
      "outputs": [],
      "source": [
        "from google.cloud.alloydb.connector import Connector, IPTypes\n",
        "import sqlalchemy\n",
        "\n",
        "\n",
        "connection_string = f\"projects/{project_id}/locations/{region}/clusters/{cluster_name}/instances/{instance_name}\"\n",
        "# initialize Connector object\n",
        "connector = Connector()\n",
        "\n",
        "\n",
        "# function to return the database connection\n",
        "def getconn():\n",
        "    conn = connector.connect(\n",
        "        connection_string,\n",
        "        \"pg8000\",\n",
        "        user=\"postgres\",\n",
        "        password=password,\n",
        "        db=database_name,\n",
        "        enable_iam_auth=False,\n",
        "        ip_type=IPTypes.PUBLIC,\n",
        "    )\n",
        "    return conn\n",
        "\n",
        "\n",
        "# create connection pool\n",
        "pool = sqlalchemy.create_engine(\"postgresql+pg8000://\", creator=getconn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdolCWyatZmG"
      },
      "source": [
        "### Import data to your database"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQ2KWsYI_Msa"
      },
      "source": [
        "The following code has been prepared code to help insert the CSV data into your AlloyDB for PostgreSQL database."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dzr-2VZIkvtY"
      },
      "source": [
        "Download the CSV file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "5KkIQ2zSvQkN"
      },
      "outputs": [],
      "source": [
        "!gsutil cp gs://twisha-dev_cloudbuild/investments_csv /content/investments.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFU13dCBlYHh"
      },
      "source": [
        "The download can be verified by the following command or using the \"Files\" tab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "nQBs10I8vShh"
      },
      "outputs": [],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2H7rorG9Ivur"
      },
      "source": [
        "In this next step you will:\n",
        "\n",
        "1. Create the table into store data\n",
        "2. And insert the data from the CSV into the database table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ouy1HvIUGbbh"
      },
      "outputs": [],
      "source": [
        "table_name = \"investments\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "qCsM2KXbdYiv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "create_table_cmd = sqlalchemy.text(\n",
        "    f'CREATE TABLE {table_name} ( \\\n",
        "    id SERIAL PRIMARY KEY, \\\n",
        "    ticker VARCHAR(255) NOT NULL UNIQUE, \\\n",
        "    etf BOOLEAN, \\\n",
        "    market VARCHAR(255), \\\n",
        "    rating TEXT,  \\\n",
        "    overview TEXT, \\\n",
        "    overview_embedding VECTOR (768), \\\n",
        "    analysis TEXT,  \\\n",
        "    analysis_embedding VECTOR (768) \\\n",
        "    )'\n",
        ")\n",
        "\n",
        "data = \"/content/investments.csv\"\n",
        "\n",
        "df = pd.read_csv(data)\n",
        "insert_data_cmd = sqlalchemy.text(\n",
        "    \"\"\"\n",
        "    INSERT INTO investments (id, ticker, etf, market,\n",
        "      rating, overview, analysis) VALUES (:id, :ticker, :etf, :market,\n",
        "      :rating, :overview, :analysis)\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "parameter_map = [\n",
        "    {\n",
        "        \"id\": row[\"id\"],\n",
        "        \"ticker\": row[\"ticker\"],\n",
        "        \"etf\": row[\"etf\"],\n",
        "        \"market\": row[\"market\"],\n",
        "        \"rating\": row[\"rating\"],\n",
        "        \"overview\": row[\"overview\"],\n",
        "        \"analysis\": row[\"analysis\"],\n",
        "    }\n",
        "    for index, row in df.iterrows()\n",
        "]\n",
        "\n",
        "with pool.connect() as db_conn:\n",
        "    db_conn.execute(create_table_cmd)\n",
        "    db_conn.execute(\n",
        "        insert_data_cmd,\n",
        "        parameter_map,\n",
        "    )\n",
        "    db_conn.commit()\n",
        "connector.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XirWZcZErpkf"
      },
      "source": [
        "### Fetch Source Data to Embed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sqlalchemy.engine.row import Row\n",
        "from typing import List"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "fwjn5tIwl4Ac"
      },
      "outputs": [],
      "source": [
        "def get_source_data() -> List[dict[str, Row]]:\n",
        "  sql = f\"\"\"SELECT id, overview, analysis FROM {table_name};\"\"\"\n",
        "  col_names = [\"id\", \"overview\", \"analysis\"]\n",
        "\n",
        "  print(f\"Running SQL query: {sql}\")\n",
        "\n",
        "  with pool.connect() as db_conn:\n",
        "    rows = db_conn.execute(sqlalchemy.text(sql))\n",
        "    source_array = [dict(zip(col_names, row)) for row in rows]\n",
        "    db_conn.commit()\n",
        "\n",
        "  connector.close()\n",
        "\n",
        "  return source_array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obF8lNLCtCk1"
      },
      "source": [
        "### Define Batching Function\n",
        "\n",
        "This helper function dynamically builds batches of text chunks to efficiently generate multiple embeddings with each call to the API.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "4wuRFoczs1_A"
      },
      "outputs": [],
      "source": [
        "max_tokens = 20000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ZWPOII1BtRMW"
      },
      "outputs": [],
      "source": [
        "# Function to build batches for embedding based on max tokens/characters\n",
        "def build_batch_array(source_array: List[dict[str, Row]], column_to_embed: str) -> List[dict[str, Row]]:\n",
        "  global index_pointer  # Assumes index_pointer is defined globally\n",
        "  global batch_char_count  # Assumes batch_char_count is defined globally\n",
        "  global total_char_count  # Assumes total_char_count is defined globally\n",
        "\n",
        "  batch_array = []\n",
        "  current_char_count = 0\n",
        "  max_char_count = max_tokens * 3  # Approximate characters per token\n",
        "\n",
        "  while current_char_count < max_char_count and index_pointer < len(source_array):\n",
        "      obj = source_array[index_pointer]\n",
        "      text_to_embed = obj[column_to_embed]\n",
        "      text_char_count = len(text_to_embed)\n",
        "\n",
        "      if current_char_count + text_char_count <= max_char_count:\n",
        "          batch_array.append(obj)\n",
        "          current_char_count += text_char_count\n",
        "          index_pointer += 1\n",
        "      else:\n",
        "          break  # Exit loop if adding the next object exceeds the limit\n",
        "\n",
        "  batch_char_count = current_char_count\n",
        "  total_char_count += batch_char_count\n",
        "\n",
        "  return batch_array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRfixxyHtb3U"
      },
      "source": [
        "### Define Embedding Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "zVlLHGi3tbdE"
      },
      "outputs": [],
      "source": [
        "from vertexai.language_models import TextEmbeddingInput, TextEmbeddingModel\n",
        "\n",
        "def embed_text(\n",
        "    texts: List[str],\n",
        "    task: str = \"SEMANTIC_SIMILARITY\",\n",
        "    model_name: str = \"textembedding-gecko@003\",\n",
        ") -> List[List[float]]:\n",
        "    model = TextEmbeddingModel.from_pretrained(model_name)\n",
        "    inputs = [TextEmbeddingInput(text, task) for text in texts]\n",
        "    embeddings = model.get_embeddings(inputs)\n",
        "    return [embedding.values for embedding in embeddings]\n",
        "\n",
        "\n",
        "def embed_objects(source_array: List[dict[str, Row]], column_to_embed: str) -> List[dict[str, Row]]:\n",
        "    global index_pointer  # Assumes index_pointer is defined globally\n",
        "    global batch_count  # Assumes batch_count is defined globally\n",
        "    global batch_char_count  # Assumes batch_char_count is defined globally\n",
        "    global total_char_count  # Assumes total_char_count is defined globally\n",
        "\n",
        "    total_objects = len(source_array)\n",
        "    print(f\"Beginning source_array size: {total_objects}\")\n",
        "\n",
        "    result_array = []\n",
        "\n",
        "    while index_pointer < total_objects:\n",
        "        batch_array = build_batch_array(source_array, column_to_embed)\n",
        "        print(\"Batch array size\", len(batch_array))\n",
        "\n",
        "        if batch_array:\n",
        "            batch_count += 1\n",
        "            print(f\"Processing batch {batch_count} with size: {len(batch_array)}. \"\n",
        "                  f\"Progress: {index_pointer} / {total_objects}. \"\n",
        "                  f\"Character count (batch): {batch_char_count}. \"\n",
        "                  f\"Character count (cumulative): {total_char_count}\")\n",
        "\n",
        "            texts_to_embed = [obj[column_to_embed] for obj in batch_array]\n",
        "            embeddings = embed_text(texts_to_embed)\n",
        "\n",
        "            for i, obj in enumerate(batch_array):\n",
        "                obj['embedding'] = embeddings[i]\n",
        "                result_array.append(obj)\n",
        "\n",
        "    return result_array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPRL22gsvAfB"
      },
      "source": [
        "### Define Bulk Load and Update Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "O8AWZ3FlvXLp"
      },
      "outputs": [],
      "source": [
        "import tempfile\n",
        "import csv\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "48GUZptz89vh"
      },
      "outputs": [],
      "source": [
        "def insert_to_temp_table(temp_table_name: str, column_to_embed: str, object_array: List[dict[str, Row]]) -> None:\n",
        "    with pool.connect() as db_conn:\n",
        "        for obj in object_array:\n",
        "            db_conn.execute(sqlalchemy.text(\n",
        "                f\"\"\"INSERT INTO {temp_table_name} (id, col_name, embedding)\n",
        "                VALUES (:id, :col_name, :embedding)\"\"\"\n",
        "            ),  {'id': obj['id'], 'col_name': column_to_embed, 'embedding': obj['embedding']})\n",
        "        db_conn.commit()\n",
        "    connector.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "pnAlCSyjtV5e"
      },
      "outputs": [],
      "source": [
        "# Functions to manage temporary table and update the target table\n",
        "def create_temp_table(column_to_embed: str) -> None:\n",
        "    temp_table_name = f\"{column_to_embed}_embeddings_temp\"\n",
        "    # Update the SQL query below to match your environment\n",
        "    sql = f\"\"\"\n",
        "    DROP TABLE IF EXISTS {temp_table_name};\n",
        "    CREATE TABLE {temp_table_name} (\n",
        "        id INTEGER PRIMARY KEY,\n",
        "        col_name TEXT,\n",
        "        embedding REAL[]\n",
        "    );\n",
        "    \"\"\"\n",
        "    with pool.connect() as db_conn:\n",
        "      db_conn.execute(sqlalchemy.text(sql))\n",
        "      db_conn.commit()\n",
        "    connector.close()\n",
        "\n",
        "    return temp_table_name\n",
        "\n",
        "\n",
        "def update_target_table(temp_table_name: str, target_table_name: str, column_to_embed: str) -> None:\n",
        "    # Update the SQL query below to match your environment\n",
        "    sql = f\"\"\"\n",
        "    UPDATE {target_table_name}\n",
        "    SET {column_to_embed}_embedding = {temp_table_name}.embedding\n",
        "    FROM {temp_table_name}\n",
        "    WHERE {target_table_name}.id = {temp_table_name}.id;\n",
        "    \"\"\"\n",
        "\n",
        "    with pool.connect() as db_conn:\n",
        "      db_conn.execute(sqlalchemy.text(sql))\n",
        "      db_conn.commit()\n",
        "    connector.close()\n",
        "\n",
        "\n",
        "def drop_temp_table(temp_table_name: str) -> None:\n",
        "    sql = f\"\"\"DROP TABLE {temp_table_name};\"\"\"\n",
        "\n",
        "    with pool.connect() as db_conn:\n",
        "      db_conn.execute(sqlalchemy.text(sql))\n",
        "      db_conn.commit()\n",
        "    connector.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzVvvKi0xeuT"
      },
      "source": [
        "### Run the embedding process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "xn9ou0J76KM0"
      },
      "outputs": [],
      "source": [
        "# Define table where embeddings will be written and columns to be embedded\n",
        "import time\n",
        "import vertexai\n",
        "\n",
        "target_table_name = table_name\n",
        "columns_to_embed = ['analysis','overview']\n",
        "\n",
        "# Define global variables to track progress and estimate cost\n",
        "global index_pointer\n",
        "global batch_count\n",
        "global batch_char_count\n",
        "global total_char_count\n",
        "\n",
        "# Define batch variables\n",
        "batch_array = None\n",
        "batch_size = None\n",
        "batch_count = 0\n",
        "total_char_count = 0\n",
        "index_pointer = 0\n",
        "\n",
        "column_to_embed = 'analysis'\n",
        "vertexai.init(project=project_id)\n",
        "\n",
        "source_array = get_source_data()\n",
        "\n",
        "# Keep track of job timing\n",
        "start_time = time.time()\n",
        "\n",
        "for column_to_embed in columns_to_embed:\n",
        "  # Initialize the index pointer for batch processing\n",
        "  index_pointer = 0\n",
        "\n",
        "  print(f\"Creating embeddings for column {column_to_embed}...\")\n",
        "  results = embed_objects(source_array, column_to_embed)\n",
        "\n",
        "  print(f\"Creating temp table to store intermediate results...\")\n",
        "  temp_table_name = create_temp_table(column_to_embed)\n",
        "\n",
        "  print(f\"Inserting embeddings into temp table: {temp_table_name}...\")\n",
        "  insert_to_temp_table(temp_table_name, column_to_embed, results)\n",
        "\n",
        "  print(f\"Merging temp table {temp_table_name} with target table {target_table_name}...\")\n",
        "  update_target_table(temp_table_name, target_table_name, column_to_embed)\n",
        "\n",
        "  print(f\"Dropping temp table temp_table_name...\")\n",
        "  drop_temp_table(temp_table_name)\n",
        "\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "print(f\"Job started at: {time.ctime(start_time)}\")\n",
        "print(f\"Job ended at: {time.ctime(end_time)}\")\n",
        "print(f\"Total run time: {elapsed_time:.2f} seconds\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
